{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorganize_and_explore.py\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use relative path from notebook location\n",
    "# This works whether you're in notebooks/, scripts/, or project root\n",
    "notebook_dir = Path.cwd()\n",
    "\n",
    "# Find project root (where pyproject.toml is)\n",
    "project_root = notebook_dir\n",
    "while not (project_root / \"pyproject.toml\").exists():\n",
    "    project_root = project_root.parent\n",
    "    if project_root == project_root.parent:  # Reached filesystem root\n",
    "        raise FileNotFoundError(\"Could not find project root (pyproject.toml not found)\")\n",
    "\n",
    "# Now use relative paths from project root\n",
    "base_dir = project_root / \"fall_detection_data\"\n",
    "processed_dir = base_dir / \"processed\"\n",
    "models_dir = base_dir / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "output_dir = models_dir\n",
    "\n",
    "print(f\"üìÇ Project root: {project_root}\")\n",
    "print(f\"üìÇ Data directory: {base_dir}\")\n",
    "print(f\"üìÇ Models directory: {models_dir}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CURRENT DIRECTORY STRUCTURE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show current structure\n",
    "for item in sorted(base_dir.iterdir()):\n",
    "    if item.is_dir():\n",
    "        print(f\"\\nüìÅ {item.name}/\")\n",
    "        # Show what's inside each directory\n",
    "        sub_items = list(item.iterdir())[:5]\n",
    "        for sub in sub_items:\n",
    "            if sub.is_dir():\n",
    "                file_count = len(list(sub.glob(\"*\")))\n",
    "                print(f\"   üìÅ {sub.name}/ ({file_count} files)\")\n",
    "            else:\n",
    "                print(f\"   üìÑ {sub.name}\")\n",
    "        if len(list(item.iterdir())) > 5:\n",
    "            print(f\"   ... and {len(list(item.iterdir())) - 5} more\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROPOSED REORGANIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "proposed_structure = \"\"\"\n",
    "fall_detection_data/\n",
    "‚îú‚îÄ‚îÄ KFall/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ sensor_data/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SA06/\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ S06T01R01.csv  (KFall format: S##T##R##.csv)\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ S06T02R01.csv\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SA07/ ...\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ labels/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ SA06_label.xlsx\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ SA07_label.xlsx ...\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ SisFall/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ SA01/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ D01_SA01_R01.txt  (SisFall format: <CODE>_<SUBJECT>_<TRIAL>.txt)\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ F01_SA01_R01.txt\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ SA02/ ...\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ SE01/ ... (elderly subjects)\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ processed/\n",
    "    ‚îú‚îÄ‚îÄ kfall_features.pkl\n",
    "    ‚îú‚îÄ‚îÄ sisfall_features.pkl\n",
    "    ‚îî‚îÄ‚îÄ fused_dataset.pkl\n",
    "\"\"\"\n",
    "\n",
    "print(proposed_structure)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATASET COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# KFall structure\n",
    "kfall_sensor = base_dir / \"KFall\" / \"sensor_data\"\n",
    "if kfall_sensor.exists():\n",
    "    kfall_subjects = sorted([d.name for d in kfall_sensor.iterdir() if d.is_dir()])\n",
    "    sample_kfall = kfall_sensor / kfall_subjects[0]\n",
    "    sample_kfall_file = list(sample_kfall.glob(\"*.csv\"))[0]\n",
    "    \n",
    "    df_kfall = pd.read_csv(sample_kfall_file)\n",
    "    \n",
    "    print(\"\\nüìä KFALL DATASET:\")\n",
    "    print(f\"   Subjects: {len(kfall_subjects)} (SA06-SA38)\")\n",
    "    print(f\"   Sampling Rate: 100 Hz (needs upsampling to 200 Hz)\")\n",
    "    print(f\"   File Format: S##T##R##.csv\")\n",
    "    print(f\"   Columns: {df_kfall.columns.tolist()}\")\n",
    "    print(f\"   Data Shape (sample): {df_kfall.shape}\")\n",
    "    print(f\"   Has Labels: ‚úÖ Yes (temporal annotations in Excel files)\")\n",
    "\n",
    "# SisFall structure\n",
    "sisfall_dir = base_dir / \"SisFall\"\n",
    "if sisfall_dir.exists():\n",
    "    sisfall_subjects = sorted([d.name for d in sisfall_dir.iterdir() if d.is_dir()])\n",
    "    adults = [s for s in sisfall_subjects if s.startswith('SA')]\n",
    "    elderly = [s for s in sisfall_subjects if s.startswith('SE')]\n",
    "    \n",
    "    sample_sisfall = sisfall_dir / adults[0]\n",
    "    sample_sisfall_file = list(sample_sisfall.glob(\"*.txt\"))[0]\n",
    "    \n",
    "    # Read SisFall file - more robust parsing\n",
    "    try:\n",
    "        # Method 1: Read line by line and parse manually\n",
    "        with open(sample_sisfall_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        data = []\n",
    "        for line in lines:\n",
    "            # Remove semicolon and split by comma or whitespace\n",
    "            line = line.strip().replace(';', '')\n",
    "            values = line.replace(',', ' ').split()\n",
    "            if len(values) == 9:  # Should have 9 columns\n",
    "                data.append([float(v) for v in values])\n",
    "        \n",
    "        df_sisfall = pd.DataFrame(data)\n",
    "        \n",
    "        print(\"\\nüìä SISFALL DATASET:\")\n",
    "        print(f\"   Subjects: {len(sisfall_subjects)} total\")\n",
    "        print(f\"     - Adults (SA): {len(adults)} (SA01-SA23)\")\n",
    "        print(f\"     - Elderly (SE): {len(elderly)} (SE01-SE15)\")\n",
    "        print(f\"   Sampling Rate: 200 Hz ‚úÖ\")\n",
    "        print(f\"   File Format: <CODE>_<SUBJECT>_<TRIAL>.txt\")\n",
    "        print(f\"   Columns: 9 (ADXL345: 0-2, ITG3200: 3-5, MMA8451Q: 6-8)\")\n",
    "        print(f\"   Data Shape (sample): {df_sisfall.shape}\")\n",
    "        print(f\"   Has Labels: ‚ùå No (must use Algorithm 1)\")\n",
    "        print(f\"   Data Format: Raw bits (needs conversion to physical units)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error reading SisFall file: {e}\")\n",
    "        print(\"   Will handle this in the preprocessing pipeline\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ACTIVITIES NEEDED FOR PAPER REPRODUCTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìã FROM KFALL (Table I):\")\n",
    "kfall_needed = {\n",
    "    'T10': 'Stumble while walking',\n",
    "    'T28': 'Vertical fall while walking (fainting)',\n",
    "    'T30': 'Forward fall while walking (trip)',\n",
    "    'T31': 'Forward fall while jogging (trip)',\n",
    "    'T32': 'Forward fall while walking (slip)',\n",
    "    'T33': 'Lateral fall while walking (slip)',\n",
    "    'T34': 'Backward fall while walking (slip)'\n",
    "}\n",
    "for code, desc in kfall_needed.items():\n",
    "    print(f\"   {code}: {desc}\")\n",
    "\n",
    "print(\"\\nüìã FROM SISFALL (Table I):\")\n",
    "print(\"\\n   ADL Activities:\")\n",
    "sisfall_adl = {\n",
    "    'D01': 'Walking slowly',\n",
    "    'D02': 'Walking quickly',\n",
    "    'D03': 'Jogging slowly',\n",
    "    'D04': 'Jogging quickly',\n",
    "    'D05': 'Walking upstairs/downstairs slowly',\n",
    "    'D06': 'Walking upstairs/downstairs quickly',\n",
    "    'D18': 'Stumble while walking'\n",
    "}\n",
    "for code, desc in sisfall_adl.items():\n",
    "    print(f\"   {code}: {desc}\")\n",
    "\n",
    "print(\"\\n   Fall Activities:\")\n",
    "sisfall_falls = {\n",
    "    'F01': 'Fall forward while walking (slip)',\n",
    "    'F02': 'Fall backward while walking (slip)',\n",
    "    'F03': 'Lateral fall while walking (slip)',\n",
    "    'F04': 'Fall forward while walking (trip)',\n",
    "    'F05': 'Fall forward while jogging (trip)',\n",
    "    'F06': 'Vertical fall while walking (fainting)'\n",
    "}\n",
    "for code, desc in sisfall_falls.items():\n",
    "    print(f\"   {code}: {desc}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "1. ‚úÖ Data is properly organized\n",
    "2. ‚è≠Ô∏è  Implement preprocessing pipeline:\n",
    "   - Load and convert SisFall raw bits to physical units\n",
    "   - Upsample KFall from 100Hz to 200Hz\n",
    "   - Apply Algorithm 1 for temporal segmentation\n",
    "   - Extract features according to Table I\n",
    "3. ‚è≠Ô∏è  Z-score normalization and dataset fusion\n",
    "4. ‚è≠Ô∏è  Build and train FallNet\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Fall Detection Data Preprocessing Pipeline - CORRECTED VERSION\n",
    "# \n",
    "# This notebook implements the preprocessing methodology from the paper:\n",
    "# \"A novel Feature extraction method for Pre-Impact Fall detection system using Deep learning and wearable sensors\"\n",
    "#\n",
    "# Key fixes:\n",
    "# - Removed Sp - 3 bug\n",
    "# - Fixed transitional window logic (no duplicates)\n",
    "# - Proper ADL extraction before falls\n",
    "# - Correct stumble/recovery processing\n",
    "\n",
    "# %% [markdown]\n",
    "## 1. Setup and Imports\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.interpolate import CubicSpline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Imports complete\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 2. Define Paths and Configuration\n",
    "\n",
    "# %%\n",
    "# Auto-detect project root (works from any directory in the project)\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "# Find project root by looking for pyproject.toml\n",
    "project_root = current_dir\n",
    "while not (project_root / \"pyproject.toml\").exists():\n",
    "    project_root = project_root.parent\n",
    "    if project_root == project_root.parent:  # Reached filesystem root\n",
    "        # Fallback: assume we're in notebooks/ directory\n",
    "        project_root = current_dir.parent if current_dir.name == \"notebooks\" else current_dir\n",
    "        break\n",
    "\n",
    "# Define paths relative to project root\n",
    "base_dir = project_root / \"fall_detection_data\"\n",
    "kfall_sensor_dir = base_dir / \"KFall\" / \"sensor_data\"\n",
    "kfall_labels_dir = base_dir / \"KFall\" / \"label_data\"\n",
    "sisfall_dir = base_dir / \"SisFall\"\n",
    "processed_dir = base_dir / \"processed\"\n",
    "\n",
    "# Verify paths exist\n",
    "if not base_dir.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Data directory not found: {base_dir}\\n\"\n",
    "        f\"Expected structure: {project_root}/fall_detection_data/\\n\"\n",
    "        f\"Current directory: {current_dir}\\n\"\n",
    "        f\"Project root: {project_root}\"\n",
    "    )\n",
    "\n",
    "# Clean up processed directory\n",
    "print(\"üßπ Cleaning processed directory...\")\n",
    "if processed_dir.exists():\n",
    "    for f in processed_dir.glob(\"*\"):\n",
    "        if f.is_file():\n",
    "            f.unlink()\n",
    "            print(f\"  Deleted: {f.name}\")\n",
    "else:\n",
    "    processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\n‚úÖ Directories configured:\")\n",
    "print(f\"   Project root: {project_root}\")\n",
    "print(f\"   Data directory: {base_dir}\")\n",
    "print(f\"   KFall sensor data: {kfall_sensor_dir}\")\n",
    "print(f\"   KFall labels: {kfall_labels_dir}\")\n",
    "print(f\"   SisFall data: {sisfall_dir}\")\n",
    "print(f\"   Output: {processed_dir}\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 3. Activity Mappings and Labels\n",
    "\n",
    "# %%\n",
    "# Activities from Table I in the paper\n",
    "kfall_fall_activities = ['T28', 'T30', 'T31', 'T32', 'T33', 'T34']\n",
    "kfall_stumble = ['T10']\n",
    "\n",
    "sisfall_adl_map = {\n",
    "    'D01': 'Walking', 'D02': 'Walking',\n",
    "    'D03': 'Jogging', 'D04': 'Jogging',\n",
    "    'D05': 'Walking_stairs_updown', 'D06': 'Walking_stairs_updown',\n",
    "    'D18': 'Stumble_while_walking'\n",
    "}\n",
    "\n",
    "sisfall_falls = ['F01', 'F02', 'F03', 'F04', 'F05', 'F06']\n",
    "\n",
    "# Label encoding (8-class classifier)\n",
    "label_map = {\n",
    "    'Walking': 0,\n",
    "    'Jogging': 1,\n",
    "    'Walking_stairs_updown': 2,\n",
    "    'Stumble_while_walking': 3,\n",
    "    'Fall_Recovery': 4,\n",
    "    'Fall_Initiation': 5,\n",
    "    'Impact': 6,\n",
    "    'Aftermath': 7\n",
    "}\n",
    "\n",
    "reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "print(\"üìã Label Mapping:\")\n",
    "for label_name, label_id in label_map.items():\n",
    "    print(f\"   {label_id}: {label_name}\")\n",
    "\n",
    "# Save label map immediately\n",
    "with open(processed_dir / \"label_map.json\", \"w\") as f:\n",
    "    json.dump(label_map, f, indent=2)\n",
    "print(\"\\n‚úÖ Label map saved\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 4. Data Loading Functions\n",
    "\n",
    "# %%\n",
    "def load_sisfall_file(filepath):\n",
    "    \"\"\"Load and convert SisFall file from bits to physical units\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    data = []\n",
    "    for line in lines:\n",
    "        line = line.strip().replace(';', '').replace(',', ' ')\n",
    "        values = line.split()\n",
    "        if len(values) == 9:\n",
    "            data.append([float(v) for v in values])\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    # Convert to physical units\n",
    "    converted = np.zeros((data.shape[0], 6))\n",
    "    \n",
    "    # ADXL345 (columns 0-2): ¬±16g, 13-bit\n",
    "    adxl_factor = (2 * 16) / (2**13)\n",
    "    converted[:, 0:3] = data[:, 0:3] * adxl_factor\n",
    "    \n",
    "    # ITG3200 (columns 3-5): ¬±2000¬∞/s, 16-bit  \n",
    "    itg_factor = (2 * 2000) / (2**16)\n",
    "    converted[:, 3:6] = data[:, 3:6] * itg_factor\n",
    "    \n",
    "    return converted\n",
    "\n",
    "def load_kfall_file(filepath):\n",
    "    \"\"\"Load KFall CSV file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        data = df[['AccX', 'AccY', 'AccZ', 'GyrX', 'GyrY', 'GyrZ']].values\n",
    "        return data\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Data loading functions defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 5. Upsampling Function (KFall 100Hz ‚Üí 200Hz)\n",
    "\n",
    "# %%\n",
    "def upsample_to_200hz(data, original_freq=100):\n",
    "    \"\"\"Upsample KFall data from 100Hz to 200Hz using cubic spline\"\"\"\n",
    "    n_samples, n_features = data.shape\n",
    "    original_time = np.arange(n_samples) / original_freq\n",
    "    target_time = np.arange(0, n_samples / original_freq, 1 / 200)\n",
    "    \n",
    "    upsampled = np.zeros((len(target_time), n_features))\n",
    "    for i in range(n_features):\n",
    "        cs = CubicSpline(original_time, data[:, i])\n",
    "        upsampled[:, i] = cs(target_time)\n",
    "    \n",
    "    return upsampled\n",
    "\n",
    "print(\"‚úÖ Upsampling function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 6. Algorithm 1: Temporal Feature Extraction (CORRECTED)\n",
    "\n",
    "# %%\n",
    "def extract_temporal_features(data, sampling_freq=200):\n",
    "    \"\"\"\n",
    "    Algorithm 1 from the paper: Automatic temporal feature extraction\n",
    "    Uses Y-axis acceleration (gravity direction)\n",
    "    \n",
    "    FIXED: Removed Sp - 3 bug\n",
    "    \"\"\"\n",
    "    acc_y = data[:, 1]  # Y-axis\n",
    "    W_s = sampling_freq // 4  # 50 samples (0.25s)\n",
    "    \n",
    "    # Calculate std on non-overlapping windows\n",
    "    std_devs = []\n",
    "    window_positions = []\n",
    "    for i in range(0, len(acc_y) - W_s, W_s):\n",
    "        window = acc_y[i:i + W_s]\n",
    "        std_devs.append(np.std(window))\n",
    "        window_positions.append(i)\n",
    "    \n",
    "    if len(std_devs) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Find segmentation point\n",
    "    # CRITICAL FIX: Don't subtract 3!\n",
    "    max_std_idx = np.argmax(std_devs)\n",
    "    Sp = max_std_idx  # Paper says: \"The starting frame of Sw will become Sp\"\n",
    "    \n",
    "    segments = {\n",
    "        'std_devs': std_devs,\n",
    "        'window_positions': window_positions,\n",
    "        'Sp': Sp,\n",
    "        'W_s': W_s,\n",
    "        \n",
    "        # Phase boundaries (in samples)\n",
    "        'adl_end': Sp * W_s,\n",
    "        'fall_init_start': Sp * W_s,\n",
    "        'fall_init_end': min((Sp + 4) * W_s, len(data)),\n",
    "        'transitional_end': min((Sp + 2) * W_s, len(data)),\n",
    "        'impact_start': min((Sp + 4) * W_s, len(data)),\n",
    "        'impact_end': min((Sp + 8) * W_s, len(data)),\n",
    "        'aftermath_start': min((Sp + 8) * W_s, len(data)),\n",
    "    }\n",
    "    \n",
    "    return segments\n",
    "\n",
    "print(\"‚úÖ Algorithm 1 implemented (CORRECTED)\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 7. Feature Extraction Functions (CORRECTED)\n",
    "\n",
    "# %%\n",
    "def process_fall_activity(data):\n",
    "    \"\"\"\n",
    "    Extract features from fall activity using Algorithm 1\n",
    "    \n",
    "    FIXED:\n",
    "    - No duplicate Fall_Initiation samples\n",
    "    - Properly extracts ADL before fall\n",
    "    - Uses transitional window for 50% of samples (random selection)\n",
    "    \"\"\"\n",
    "    segments = extract_temporal_features(data)\n",
    "    if segments is None:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    W_s = segments['W_s']\n",
    "    \n",
    "    # 1. ADL phase (before fall) - if available\n",
    "    adl_start = max(0, segments['fall_init_start'] - 200)\n",
    "    adl_end = segments['fall_init_start']\n",
    "    \n",
    "    if adl_end - adl_start >= 200 and adl_start >= 0:\n",
    "        adl_segment = data[adl_start:adl_end]\n",
    "        \n",
    "        # Determine ADL type based on variance\n",
    "        acc_std = np.std(adl_segment[:, 1])\n",
    "        if acc_std > 0.5:\n",
    "            adl_label = label_map['Jogging']\n",
    "        else:\n",
    "            adl_label = label_map['Walking']\n",
    "        \n",
    "        results.append((adl_segment[:200], adl_label))\n",
    "    \n",
    "    # 2. Fall Initiation - ONE sample per fall\n",
    "    # Randomly choose between transitional window (0.5s) or full window (1s)\n",
    "    fi_start = segments['fall_init_start']\n",
    "    \n",
    "    if np.random.random() < 0.5:\n",
    "        # Use transitional window (0.5s) for early detection training\n",
    "        tw_end = segments['transitional_end']\n",
    "        if tw_end <= len(data) and (tw_end - fi_start) >= 100:\n",
    "            tw_segment = data[fi_start:tw_end]\n",
    "            \n",
    "            # Interpolate to 200 samples\n",
    "            if len(tw_segment) != 200:\n",
    "                time_orig = np.linspace(0, 1, len(tw_segment))\n",
    "                time_new = np.linspace(0, 1, 200)\n",
    "                tw_interp = np.zeros((200, 6))\n",
    "                for i in range(6):\n",
    "                    tw_interp[:, i] = np.interp(time_new, time_orig, tw_segment[:, i])\n",
    "                results.append((tw_interp, label_map['Fall_Initiation']))\n",
    "            else:\n",
    "                results.append((tw_segment, label_map['Fall_Initiation']))\n",
    "    else:\n",
    "        # Use full Fall Initiation window (1s)\n",
    "        fi_end = segments['fall_init_end']\n",
    "        if fi_end <= len(data) and (fi_end - fi_start) >= 200:\n",
    "            fi_segment = data[fi_start:fi_end]\n",
    "            results.append((fi_segment[:200], label_map['Fall_Initiation']))\n",
    "    \n",
    "    # 3. Impact\n",
    "    impact_start = segments['impact_start']\n",
    "    impact_end = segments['impact_end']\n",
    "    if impact_end <= len(data) and (impact_end - impact_start) >= 200:\n",
    "        impact_segment = data[impact_start:impact_end]\n",
    "        results.append((impact_segment[:200], label_map['Impact']))\n",
    "    \n",
    "    # 4. Aftermath\n",
    "    aftermath_start = segments['aftermath_start']\n",
    "    if len(data) - aftermath_start >= 200:\n",
    "        aftermath_segment = data[aftermath_start:aftermath_start + 200]\n",
    "        results.append((aftermath_segment, label_map['Aftermath']))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def process_stumble_activity(data):\n",
    "    \"\"\"\n",
    "    Process stumble/fall recovery\n",
    "    \n",
    "    Stumble = temporary loss of balance WITHOUT falling (recovers)\n",
    "    \"\"\"\n",
    "    segments = extract_temporal_features(data)\n",
    "    if segments is None:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Extract the \"stumble\" moment (the imbalance event)\n",
    "    stumble_start = segments['fall_init_start']\n",
    "    stumble_end = segments['transitional_end']\n",
    "    \n",
    "    if stumble_end <= len(data) and (stumble_end - stumble_start) >= 100:\n",
    "        stumble_segment = data[stumble_start:stumble_end]\n",
    "        \n",
    "        # Interpolate to 200 samples if needed\n",
    "        if len(stumble_segment) < 200:\n",
    "            time_orig = np.linspace(0, 1, len(stumble_segment))\n",
    "            time_new = np.linspace(0, 1, 200)\n",
    "            stumble_interp = np.zeros((200, 6))\n",
    "            for i in range(6):\n",
    "                stumble_interp[:, i] = np.interp(time_new, time_orig, stumble_segment[:, i])\n",
    "            results.append((stumble_interp, label_map['Stumble_while_walking']))\n",
    "        else:\n",
    "            results.append((stumble_segment[:200], label_map['Stumble_while_walking']))\n",
    "    \n",
    "    # Fall recovery - the recovery period after stumble\n",
    "    recovery_start = segments['transitional_end']\n",
    "    recovery_end = segments['impact_end']\n",
    "    \n",
    "    if recovery_end <= len(data) and (recovery_end - recovery_start) >= 200:\n",
    "        recovery_segment = data[recovery_start:recovery_end]\n",
    "        results.append((recovery_segment[:200], label_map['Fall_Recovery']))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def process_adl_activity(data, label_name):\n",
    "    \"\"\"\n",
    "    Extract 1-second non-overlapping windows from ADL activities\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    label = label_map[label_name]\n",
    "    \n",
    "    # Extract up to 20 seconds (as per paper)\n",
    "    max_samples = min(len(data), 4000)  # 20 seconds at 200Hz\n",
    "    \n",
    "    # Non-overlapping 1-second windows\n",
    "    for i in range(0, max_samples - 200, 200):\n",
    "        segment = data[i:i + 200]\n",
    "        if len(segment) == 200:\n",
    "            results.append((segment, label))\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Feature extraction functions defined (CORRECTED)\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 8. Process KFall Dataset\n",
    "\n",
    "# %%\n",
    "def process_kfall_dataset():\n",
    "    \"\"\"Process all KFall data\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"PROCESSING KFALL DATASET\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X_data = []\n",
    "    y_labels = []\n",
    "    \n",
    "    subjects = sorted([d for d in kfall_sensor_dir.iterdir() if d.is_dir()])\n",
    "    print(f\"Found {len(subjects)} subjects\")\n",
    "    \n",
    "    for subject_dir in tqdm(subjects, desc=\"Processing KFall subjects\"):\n",
    "        files = list(subject_dir.glob(\"*.csv\"))\n",
    "        \n",
    "        for file in files:\n",
    "            # Extract activity code from filename: S06T10R01.csv -> T10\n",
    "            filename = file.stem\n",
    "            if len(filename) < 6:\n",
    "                continue\n",
    "            activity_code = filename[3:6]  # e.g., T10, T28\n",
    "            \n",
    "            # Load and upsample\n",
    "            data = load_kfall_file(file)\n",
    "            if data is None or len(data) < 100:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                data_upsampled = upsample_to_200hz(data)\n",
    "                \n",
    "                # Process based on activity type\n",
    "                if activity_code in kfall_fall_activities:\n",
    "                    features = process_fall_activity(data_upsampled)\n",
    "                elif activity_code in kfall_stumble:\n",
    "                    features = process_stumble_activity(data_upsampled)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                for segment, label in features:\n",
    "                    if segment.shape == (200, 6):\n",
    "                        X_data.append(segment)\n",
    "                        y_labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {file.name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return np.array(X_data), np.array(y_labels)\n",
    "\n",
    "# Run KFall processing\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "X_kfall, y_kfall = process_kfall_dataset()\n",
    "print(f\"\\n‚úÖ KFall processed:\")\n",
    "print(f\"   X shape: {X_kfall.shape}\")\n",
    "print(f\"   y shape: {y_kfall.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 9. Process SisFall Dataset\n",
    "\n",
    "# %%\n",
    "def process_sisfall_dataset():\n",
    "    \"\"\"Process all SisFall data\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"PROCESSING SISFALL DATASET\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X_data = []\n",
    "    y_labels = []\n",
    "    \n",
    "    subjects = sorted([d for d in sisfall_dir.iterdir() \n",
    "                      if d.is_dir() and (d.name.startswith('SA') or d.name.startswith('SE'))])\n",
    "    print(f\"Found {len(subjects)} subjects\")\n",
    "    \n",
    "    for subject_dir in tqdm(subjects, desc=\"Processing SisFall subjects\"):\n",
    "        files = list(subject_dir.glob(\"*.txt\"))\n",
    "        \n",
    "        for file in files:\n",
    "            # Extract activity code: D01_SA01_R01.txt -> D01\n",
    "            filename = file.stem\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            activity_code = parts[0]\n",
    "            \n",
    "            # Load data\n",
    "            data = load_sisfall_file(file)\n",
    "            if data is None or len(data) < 200:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Process based on activity type\n",
    "                if activity_code in sisfall_adl_map:\n",
    "                    label_name = sisfall_adl_map[activity_code]\n",
    "                    features = process_adl_activity(data, label_name)\n",
    "                elif activity_code in sisfall_falls:\n",
    "                    features = process_fall_activity(data)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                for segment, label in features:\n",
    "                    if segment.shape == (200, 6):\n",
    "                        X_data.append(segment)\n",
    "                        y_labels.append(label)\n",
    "            except Exception as e:\n",
    "                # Silently skip problematic files\n",
    "                continue\n",
    "    \n",
    "    return np.array(X_data), np.array(y_labels)\n",
    "\n",
    "# Run SisFall processing\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "X_sisfall, y_sisfall = process_sisfall_dataset()\n",
    "print(f\"\\n‚úÖ SisFall processed:\")\n",
    "print(f\"   X shape: {X_sisfall.shape}\")\n",
    "print(f\"   y shape: {y_sisfall.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 10. Z-Score Normalization and Dataset Fusion\n",
    "\n",
    "# %%\n",
    "def normalize_and_fuse(X_kfall, y_kfall, X_sisfall, y_sisfall):\n",
    "    \"\"\"\n",
    "    Z-score normalization and dataset fusion (paper methodology)\n",
    "    \n",
    "    Paper says: \"Z-score standardization was again employed before the \n",
    "    final data was fed into the network to normalize the features extracted \n",
    "    from the two datasets\"\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"NORMALIZING AND FUSING DATASETS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Reshape for normalization\n",
    "    n_kfall, ts, feat = X_kfall.shape\n",
    "    X_kfall_flat = X_kfall.reshape(-1, feat)\n",
    "    \n",
    "    n_sisfall = X_sisfall.shape[0]\n",
    "    X_sisfall_flat = X_sisfall.reshape(-1, feat)\n",
    "    \n",
    "    print(f\"\\nStep 1: Normalize each dataset separately\")\n",
    "    # Normalize KFall\n",
    "    scaler_kfall = StandardScaler()\n",
    "    X_kfall_norm = scaler_kfall.fit_transform(X_kfall_flat)\n",
    "    X_kfall_norm = X_kfall_norm.reshape(n_kfall, ts, feat)\n",
    "    print(f\"  KFall normalized: {X_kfall_norm.shape}\")\n",
    "    \n",
    "    # Normalize SisFall\n",
    "    scaler_sisfall = StandardScaler()\n",
    "    X_sisfall_norm = scaler_sisfall.fit_transform(X_sisfall_flat)\n",
    "    X_sisfall_norm = X_sisfall_norm.reshape(n_sisfall, ts, feat)\n",
    "    print(f\"  SisFall normalized: {X_sisfall_norm.shape}\")\n",
    "    \n",
    "    print(f\"\\nStep 2: Fuse datasets\")\n",
    "    # Fuse\n",
    "    X_fused = np.concatenate([X_kfall_norm, X_sisfall_norm], axis=0)\n",
    "    y_fused = np.concatenate([y_kfall, y_sisfall], axis=0)\n",
    "    print(f\"  Fused dataset: {X_fused.shape}\")\n",
    "    \n",
    "    print(f\"\\nStep 3: Normalize fused dataset\")\n",
    "    # Final normalization\n",
    "    X_fused_flat = X_fused.reshape(-1, feat)\n",
    "    scaler_final = StandardScaler()\n",
    "    X_fused_norm = scaler_final.fit_transform(X_fused_flat)\n",
    "    X_fused_norm = X_fused_norm.reshape(-1, ts, feat)\n",
    "    print(f\"  Final normalized: {X_fused_norm.shape}\")\n",
    "    \n",
    "    # Verify normalization\n",
    "    print(f\"\\nVerification:\")\n",
    "    print(f\"  Mean: {X_fused_norm.mean():.6f} (should be ~0)\")\n",
    "    print(f\"  Std:  {X_fused_norm.std():.6f} (should be ~1)\")\n",
    "    \n",
    "    return X_fused_norm, y_fused, scaler_final\n",
    "\n",
    "# Normalize and fuse\n",
    "X_final, y_final, scaler = normalize_and_fuse(X_kfall, y_kfall, X_sisfall, y_sisfall)\n",
    "\n",
    "print(f\"\\n‚úÖ Final fused dataset:\")\n",
    "print(f\"   X shape: {X_final.shape}\")\n",
    "print(f\"   y shape: {y_final.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 11. Verify Data Quality\n",
    "\n",
    "# %%\n",
    "from collections import Counter\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA QUALITY VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Class distribution\n",
    "counts = Counter(y_final)\n",
    "print(\"\\n1. Class Distribution:\")\n",
    "for cls_id in sorted(counts.keys()):\n",
    "    count = counts[cls_id]\n",
    "    pct = count / len(y_final) * 100\n",
    "    print(f\"   {cls_id}: {reverse_label_map[cls_id]:30s} - {count:5d} ({pct:5.2f}%)\")\n",
    "\n",
    "print(f\"\\n   Total samples: {len(y_final)}\")\n",
    "\n",
    "# 2. CRITICAL: Variance test\n",
    "print(\"\\n2. Variance Test (Acc-Y axis):\")\n",
    "print(f\"   {'Class':<35s} {'Variance':<12s}\")\n",
    "print(f\"   {'-'*50}\")\n",
    "\n",
    "variances = []\n",
    "for cls_id in sorted(counts.keys()):\n",
    "    class_samples = X_final[y_final == cls_id]\n",
    "    var = class_samples[:, :, 1].var()\n",
    "    variances.append((reverse_label_map[cls_id], var))\n",
    "    print(f\"   {reverse_label_map[cls_id]:<35s} {var:>10.4f}\")\n",
    "\n",
    "# Sort by variance\n",
    "variances_sorted = sorted(variances, key=lambda x: x[1], reverse=True)\n",
    "print(f\"\\n3. Variance Ranking:\")\n",
    "for i, (name, var) in enumerate(variances_sorted, 1):\n",
    "    print(f\"   {i}. {name:<35s}: {var:.4f}\")\n",
    "\n",
    "# Check if Fall_Initiation is in top 2\n",
    "fall_init_rank = next(i for i, (name, _) in enumerate(variances_sorted, 1) if name == 'Fall_Initiation')\n",
    "\n",
    "if fall_init_rank <= 2:\n",
    "    print(f\"\\n‚úÖ PASS: Fall_Initiation ranked #{fall_init_rank} (should be #1 or #2)\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå FAIL: Fall_Initiation ranked #{fall_init_rank} (should be #1 or #2)\")\n",
    "    print(\"   Segmentation may be incorrect!\")\n",
    "\n",
    "# 4. Check for NaN/Inf\n",
    "print(f\"\\n4. Data Integrity:\")\n",
    "print(f\"   NaN values: {np.isnan(X_final).sum()}\")\n",
    "print(f\"   Inf values: {np.isinf(X_final).sum()}\")\n",
    "\n",
    "# 5. Shape verification\n",
    "print(f\"\\n5. Shape Verification:\")\n",
    "print(f\"   Expected: (N, 200, 6)\")\n",
    "print(f\"   Actual:   {X_final.shape}\")\n",
    "print(f\"   ‚úÖ PASS\" if X_final.shape[1:] == (200, 6) else \"   ‚ùå FAIL\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 12. Visualize Samples\n",
    "\n",
    "# %%\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for class_id in range(8):\n",
    "    class_indices = np.where(y_final == class_id)[0]\n",
    "    if len(class_indices) > 0:\n",
    "        sample_idx = class_indices[0]\n",
    "        sample_data = X_final[sample_idx]\n",
    "        \n",
    "        time = np.arange(200) / 200\n",
    "        axes[class_id].plot(time, sample_data[:, 0], label='Acc-X', alpha=0.7, linewidth=1)\n",
    "        axes[class_id].plot(time, sample_data[:, 1], label='Acc-Y', alpha=0.7, linewidth=1)\n",
    "        axes[class_id].plot(time, sample_data[:, 2], label='Acc-Z', alpha=0.7, linewidth=1)\n",
    "        \n",
    "        axes[class_id].set_title(f'Class {class_id}: {reverse_label_map[class_id]}', \n",
    "                                fontsize=11, fontweight='bold')\n",
    "        axes[class_id].set_xlabel('Time (s)')\n",
    "        axes[class_id].set_ylabel('Normalized Acc')\n",
    "        axes[class_id].legend(loc='upper right', fontsize=8)\n",
    "        axes[class_id].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Sample Segments from Each Activity Class', \n",
    "             fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(processed_dir / 'sample_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Visualization saved to {processed_dir / 'sample_visualization.png'}\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 13. Save Processed Data\n",
    "\n",
    "# %%\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING PROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save arrays\n",
    "np.save(processed_dir / \"X_data.npy\", X_final)\n",
    "np.save(processed_dir / \"y_labels.npy\", y_final)\n",
    "\n",
    "# Save scaler\n",
    "with open(processed_dir / \"scaler.pkl\", 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save label map (both formats)\n",
    "with open(processed_dir / \"label_map.pkl\", 'wb') as f:\n",
    "    pickle.dump(label_map, f)\n",
    "\n",
    "with open(processed_dir / \"label_map.json\", 'w') as f:\n",
    "    json.dump(label_map, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved to {processed_dir}/\")\n",
    "print(f\"   üìÑ X_data.npy: {X_final.shape}\")\n",
    "print(f\"   üìÑ y_labels.npy: {y_final.shape}\")\n",
    "print(f\"   üìÑ scaler.pkl\")\n",
    "print(f\"   üìÑ label_map.pkl\")\n",
    "print(f\"   üìÑ label_map.json\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 14. Final Summary\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING PIPELINE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "‚úÖ Successfully processed {len(y_final):,} samples\n",
    "\n",
    "Dataset Breakdown:\n",
    "  - KFall samples: {len(y_kfall):,}\n",
    "  - SisFall samples: {len(y_sisfall):,}\n",
    "  \n",
    "Data Shape:\n",
    "  - Features: {X_final.shape}\n",
    "  - Labels: {y_final.shape}\n",
    "  \n",
    "Normalization:\n",
    "  - Mean: {X_final.mean():.6f}\n",
    "  - Std: {X_final.std():.6f}\n",
    "  \n",
    "Quality Check:\n",
    "  - Fall_Initiation rank: #{fall_init_rank} (should be ‚â§2)\n",
    "  - Status: {'‚úÖ READY FOR TRAINING' if fall_init_rank <= 2 else '‚ùå NEEDS REVIEW'}\n",
    "\n",
    "Next Steps:\n",
    "  1. Load data with: X = np.load('{processed_dir}/X_data.npy')\n",
    "  2. Train FallNet model\n",
    "  3. Evaluate on stratified K-fold cross-validation\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the newly processed data\n",
    "X_data = np.load(processed_dir / \"X_data.npy\")\n",
    "y_labels = np.load(processed_dir / \"y_labels.npy\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Merge Impact and Aftermath\n",
    "# ============================================================================\n",
    "print(\"Merging Impact and Aftermath classes...\")\n",
    "y_labels[y_labels == 7] = 6  # Change Aftermath (7) to Impact (6)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Remove Fall_Recovery (NEW!)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REMOVING FALL_RECOVERY CLASS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Show before\n",
    "counts_before = Counter(y_labels)\n",
    "print(f\"\\nBefore removal:\")\n",
    "print(f\"  Total samples: {len(y_labels):,}\")\n",
    "print(f\"  Fall_Recovery (class 4): {counts_before[4]} samples\")\n",
    "\n",
    "# Remove Fall_Recovery (class 4)\n",
    "mask = y_labels != 4\n",
    "X_data = X_data[mask]\n",
    "y_labels_temp = y_labels[mask]\n",
    "\n",
    "removed_count = (~mask).sum()\n",
    "print(f\"\\n‚úÖ Removed {removed_count} Fall_Recovery samples\")\n",
    "\n",
    "# Shift labels down (5‚Üí4, 6‚Üí5)\n",
    "y_labels = y_labels_temp.copy()\n",
    "y_labels[y_labels_temp > 4] -= 1  # Classes 5,6 become 4,5\n",
    "\n",
    "print(f\"\\nAfter removal:\")\n",
    "print(f\"  Total samples: {len(y_labels):,}\")\n",
    "print(f\"  Removed: {removed_count} samples ({removed_count/(len(y_labels)+removed_count)*100:.2f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Update label map (NOW 6 CLASSES: 0-5)\n",
    "# ============================================================================\n",
    "label_map = {\n",
    "    'Walking': 0,\n",
    "    'Jogging': 1,\n",
    "    'Walking_stairs_updown': 2,\n",
    "    'Stumble_while_walking': 3,\n",
    "    'Fall_Initiation': 4,      # Was 5, now 4 ‚Üê SHIFTED DOWN!\n",
    "    'Impact_Aftermath': 5,     # Was 6, now 5 ‚Üê SHIFTED DOWN!\n",
    "}\n",
    "reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "print(f\"\\n‚úÖ Updated to 6 classes (0-5):\")\n",
    "for name, idx in sorted(label_map.items(), key=lambda x: x[1]):\n",
    "    print(f\"  Class {idx}: {name}\")\n",
    "\n",
    "y_categorical = keras.utils.to_categorical(y_labels, num_classes=6)  # ‚Üê HERE!\n",
    "print(f\"y_categorical shape: {y_categorical.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# DIAGNOSTICS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"POST-REMOVAL DATA DIAGNOSTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Class distribution\n",
    "class_counts = Counter(y_labels)\n",
    "print(\"\\n1. Class Distribution (6 classes):\")\n",
    "for cls_idx in sorted(class_counts.keys()):\n",
    "    count = class_counts[cls_idx]\n",
    "    pct = count / len(y_labels) * 100\n",
    "    print(f\"   Class {cls_idx} ({reverse_label_map[cls_idx]:30s}): {count:5d} ({pct:5.2f}%)\")\n",
    "\n",
    "# Calculate imbalance\n",
    "max_count = max(class_counts.values())\n",
    "min_count = min(class_counts.values())\n",
    "print(f\"\\nImbalance ratio: {max_count/min_count:.2f}x (was 36.8x with Fall_Recovery)\")\n",
    "\n",
    "# 2. Per-class signal statistics\n",
    "print(\"\\n2. Per-Class Signal Statistics (Acc-Y axis):\")\n",
    "print(f\"   {'Class':<35s} {'Mean':<10s} {'Std':<10s} {'Min':<10s} {'Max':<10s}\")\n",
    "print(f\"   {'-'*75}\")\n",
    "for cls_idx in sorted(class_counts.keys()):\n",
    "    class_samples = X_data[y_labels == cls_idx]\n",
    "    acc_y = class_samples[:, :, 1]  # Y-axis acceleration\n",
    "    \n",
    "    mean_val = acc_y.mean()\n",
    "    std_val = acc_y.std()\n",
    "    min_val = acc_y.min()\n",
    "    max_val = acc_y.max()\n",
    "    \n",
    "    print(f\"   {reverse_label_map[cls_idx]:<35s} {mean_val:>8.4f}  {std_val:>8.4f}  {min_val:>8.2f}  {max_val:>8.2f}\")\n",
    "\n",
    "# 3. Variance ranking\n",
    "print(\"\\n3. Variance Ranking (Fall_Initiation should be #1):\")\n",
    "variances = []\n",
    "for cls_idx in sorted(class_counts.keys()):\n",
    "    class_samples = X_data[y_labels == cls_idx]\n",
    "    acc_y_var = class_samples[:, :, 1].var()\n",
    "    variances.append((reverse_label_map[cls_idx], acc_y_var, cls_idx))\n",
    "variances.sort(key=lambda x: x[1], reverse=True)\n",
    "for i, (name, var, idx) in enumerate(variances, 1):\n",
    "    print(f\"   {i}. {name:<35s}: {var:.4f}\")\n",
    "\n",
    "# 4. Visualize samples (update to 6 classes)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "critical_classes = [\n",
    "    label_map['Walking'],\n",
    "    label_map['Fall_Initiation'],\n",
    "    label_map['Impact_Aftermath'],\n",
    "    label_map['Stumble_while_walking'],\n",
    "    label_map['Jogging'],\n",
    "    label_map['Walking_stairs_updown']\n",
    "]\n",
    "for i, cls_idx in enumerate(critical_classes):\n",
    "    if cls_idx in class_counts:\n",
    "        sample_idx = np.where(y_labels == cls_idx)[0][0]\n",
    "        sample_data = X_data[sample_idx]\n",
    "        \n",
    "        time = np.arange(200) / 200\n",
    "        axes[i].plot(time, sample_data[:, 0], label='Acc-X', alpha=0.7, linewidth=1)\n",
    "        axes[i].plot(time, sample_data[:, 1], label='Acc-Y', alpha=0.7, linewidth=1)\n",
    "        axes[i].plot(time, sample_data[:, 2], label='Acc-Z', alpha=0.7, linewidth=1)\n",
    "        \n",
    "        axes[i].set_title(f'{reverse_label_map[cls_idx]}', fontsize=11, fontweight='bold')\n",
    "        axes[i].set_xlabel('Time (s)')\n",
    "        axes[i].set_ylabel('Normalized Acc')\n",
    "        axes[i].legend(fontsize=8)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ DATA READY FOR TRAINING (6 CLASSES)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Save Preprocessed 6-Class Data\n",
    "# Save the cleaned dataset after merging Impact/Aftermath and removing Fall_Recovery\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths (same as before)\n",
    "base_dir = Path(\"~/repos/summerschool2023/projects/fall-detection/fall_detection_data\").expanduser()\n",
    "processed_dir = base_dir / \"processed\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING PREPROCESSED 6-CLASS DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save the processed data\n",
    "save_path_X = processed_dir / \"X_data_6class.npy\"\n",
    "save_path_y = processed_dir / \"y_labels_6class.npy\"\n",
    "save_path_y_cat = processed_dir / \"y_categorical_6class.npy\"\n",
    "\n",
    "np.save(save_path_X, X_data)\n",
    "np.save(save_path_y, y_labels)\n",
    "np.save(save_path_y_cat, y_categorical)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved preprocessed data:\")\n",
    "print(f\"   X_data:        {save_path_X}\")\n",
    "print(f\"   y_labels:      {save_path_y}\")\n",
    "print(f\"   y_categorical: {save_path_y_cat}\")\n",
    "\n",
    "print(f\"\\nSaved shapes:\")\n",
    "print(f\"   X_data:        {X_data.shape}\")\n",
    "print(f\"   y_labels:      {y_labels.shape}\")\n",
    "print(f\"   y_categorical: {y_categorical.shape}\")\n",
    "\n",
    "# Also save the label mapping for future reference\n",
    "label_map_path = processed_dir / \"label_map_6class.npy\"\n",
    "np.save(label_map_path, label_map)\n",
    "     # ‚úÖ Labels (numbers 0-5)\n",
    "\n",
    "# Save the LABEL MAPPING (dictionary)\n",
    "import json\n",
    "with open(processed_dir / \"label_map_6class.json\", 'w') as f:\n",
    "    json.dump(label_map, f, indent=2)                          # ‚úÖ Class names ‚Üí numbers\n",
    "print(f\"\\n   label_map:     {label_map_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ALL DATA SAVED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTo load this data in future notebooks:\")\n",
    "print(\"```python\")\n",
    "print(\"X_data = np.load(processed_dir / 'X_data_6class.npy')\")\n",
    "print(\"y_labels = np.load(processed_dir / 'y_labels_6class.npy')\")\n",
    "print(\"y_categorical = np.load(processed_dir / 'y_categorical_6class.npy')\")\n",
    "print(\"label_map = np.load(processed_dir / 'label_map_6class.npy', allow_pickle=True).item()\")\n",
    "print(\"```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # FallNet Training Pipeline\n",
    "# CNN-LMU ensemble for fall detection with 6 classes\n",
    "\n",
    "# %%\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras_lmu import LMU\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 1. FallNet Model Architecture\n",
    "\n",
    "# %%\n",
    "class FallNet:\n",
    "    \"\"\"\n",
    "    FallNet: CNN-LmU Ensemble for Pre-Impact Fall Detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape=(200, 6), n_classes=6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_shape: (timesteps, features) = (200, 6)\n",
    "            n_classes: Number of output classes (6)\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.n_classes = n_classes\n",
    "        self.model = None\n",
    "    \n",
    "    def build_lmu_branch(self, inputs):\n",
    "    # Leaner, more focused LMU\n",
    "        x = LMU(\n",
    "            memory_d=2,       # Increased for better feature separation\n",
    "            order=64,          # Reduced for smoother temporal curves\n",
    "            theta=200.0,\n",
    "            hidden_cell=layers.LSTMCell(128), # Adds nonlinear processing\n",
    "            kernel_regularizer=keras.regularizers.l2(1e-6), # L2 only\n",
    "            dropout=0.2,       # Reduced\n",
    "            name='lmu'\n",
    "        )(inputs)\n",
    "\n",
    "        # Wider Dense layer to interpret the memory\n",
    "        x = layers.Dense(512, activation='relu', name='lmu_dense1')(x)\n",
    "        x = layers.BatchNormalization()(x) # Speeds up conversion\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "        x = layers.Dense(128, activation='relu', name='lmu_dense2')(x)\n",
    "    \n",
    "        lmu_output = layers.Dense(self.n_classes, activation='softmax')(x)\n",
    "        return lmu_output\n",
    "        \n",
    "\n",
    "    def build_lstm_branch(self, inputs):\n",
    "        \"\"\"LSTM Branch\"\"\"\n",
    "        x = layers.LSTM(\n",
    "            units=256,\n",
    "            activation='tanh',\n",
    "            return_sequences=False,\n",
    "            name='lstm_layer'\n",
    "        )(inputs)\n",
    "        \n",
    "        x = layers.Dense(128, activation='relu', name='lstm_dense1')(x)\n",
    "        x = layers.Dropout(0.2, name='lstm_dropout1')(x)\n",
    "        \n",
    "        x = layers.Dense(64, activation='relu', name='lstm_dense2')(x)\n",
    "        x = layers.Dropout(0.2, name='lstm_dropout2')(x)\n",
    "        \n",
    "        x = layers.Dense(32, activation='relu', name='lstm_dense3')(x)\n",
    "        x = layers.Dropout(0.2, name='lstm_dropout3')(x)\n",
    "        \n",
    "        lstm_output = layers.Dense(\n",
    "            self.n_classes, \n",
    "            activation='softmax',\n",
    "            name='lstm_output'\n",
    "        )(x)\n",
    "        \n",
    "        return lstm_output\n",
    "    \n",
    "    def build_cnn_branch(self, inputs):\n",
    "        \"\"\"CNN Branch\"\"\"\n",
    "        x = layers.Conv1D(\n",
    "            filters=128,\n",
    "            kernel_size=3,\n",
    "            activation='relu',\n",
    "            padding='same',\n",
    "            name='conv1d_layer'\n",
    "        )(inputs)\n",
    "        \n",
    "        x = layers.MaxPooling1D(pool_size=2, name='maxpool_layer')(x)\n",
    "        \n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        x = layers.Dropout(0.2, name='cnn_dropout1')(x)\n",
    "        \n",
    "        x = layers.Dense(64, activation='relu', name='cnn_dense2')(x)\n",
    "        x = layers.Dropout(0.2, name='cnn_dropout2')(x)\n",
    "        \n",
    "        cnn_output = layers.Dense(\n",
    "            self.n_classes,\n",
    "            activation='softmax',\n",
    "            name='cnn_output'\n",
    "        )(x)\n",
    "        \n",
    "        return cnn_output\n",
    "\n",
    "    def build_cnn_only(self):\n",
    "        \"\"\"Build CNN-only model (no temporal component)\"\"\"\n",
    "        inputs = layers.Input(shape=self.input_shape, name='input')\n",
    "        cnn_output = self.build_cnn_branch(inputs)\n",
    "        \n",
    "        self.model = models.Model(\n",
    "            inputs=inputs,\n",
    "            outputs=cnn_output,\n",
    "            name='FallNet_CNN_Only'\n",
    "        )\n",
    "        return self.model\n",
    "    \n",
    "    def build_lstm_only(self):\n",
    "        \"\"\"Build LSTM-only model (temporal encoding via gates)\"\"\"\n",
    "        inputs = layers.Input(shape=self.input_shape, name='input')\n",
    "        lstm_output = self.build_lstm_branch(inputs)\n",
    "        \n",
    "        self.model = models.Model(\n",
    "            inputs=inputs,\n",
    "            outputs=lstm_output,\n",
    "            name='FallNet_LSTM_Only'\n",
    "        )\n",
    "        return self.model\n",
    "    \n",
    "    def build_lmu_only(self):\n",
    "        \"\"\"Build LMU-only model (temporal encoding via Legendre polynomials)\"\"\"\n",
    "        inputs = layers.Input(shape=self.input_shape, name='input')\n",
    "        lmu_output = self.build_lmu_branch(inputs)\n",
    "        \n",
    "        self.model = models.Model(\n",
    "            inputs=inputs,\n",
    "            outputs=lmu_output,\n",
    "            name='FallNet_LMU_Only'\n",
    "        )\n",
    "        return self.model\n",
    "    \n",
    "    def build_ensemble(self):\n",
    "        \"\"\"Build the complete ensemble model\"\"\"\n",
    "        inputs = layers.Input(shape=self.input_shape, name='input')\n",
    "        \n",
    "        lmu_output = self.build_lmu_branch(inputs)\n",
    "        cnn_output = self.build_cnn_branch(inputs)\n",
    "        \n",
    "        ensemble_output = layers.Average(name='ensemble_average')([lmu_output, cnn_output])\n",
    "        \n",
    "        self.model = models.Model(\n",
    "            inputs=inputs,\n",
    "            outputs=ensemble_output,\n",
    "            name='FallNet_CNN_LSTM'\n",
    "        )\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def compile_model(self, learning_rate=None):\n",
    "        \"\"\"Compile model\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built yet. Call build_ensemble() first.\")\n",
    "        \n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate) if learning_rate else keras.optimizers.Adam()\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=5e-4),  # 100x smaller\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "            metrics=[\n",
    "                'accuracy', \n",
    "                keras.metrics.Precision(name='precision'),\n",
    "                keras.metrics.Recall(name='recall')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "print(\"‚úÖ FallNet class defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 2. Build and Display Model\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BUILDING FALLNET MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    fallnet = FallNet(input_shape=(200, 6), n_classes=6) # Updated to 7 classes!\n",
    "    model = fallnet.build_ensemble()\n",
    "\n",
    "\n",
    "# Compile\n",
    "model = fallnet.compile_model()\n",
    "\n",
    "# Display architecture\n",
    "print(\"\\n\")\n",
    "model.summary()\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    trainable = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    non_trainable = np.sum([np.prod(v.shape) for v in model.non_trainable_weights])\n",
    "    return trainable, non_trainable\n",
    "\n",
    "trainable, non_trainable = count_parameters(model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PARAMETERS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Trainable:     {trainable:,}\")\n",
    "print(f\"Non-trainable: {non_trainable:,}\")\n",
    "print(f\"Total:         {trainable + non_trainable:,}\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 3. Training Configuration\n",
    "\n",
    "# %%\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "K_FOLDS = 5\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Max epochs: {EPOCHS}\")\n",
    "print(f\"K-Folds:    {K_FOLDS}\")\n",
    "print(f\"Using data from previous cell (6 classes, {len(y_labels):,} samples)\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 4. Verify Data Before Training\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRE-TRAINING VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"‚úÖ Data shapes:\")\n",
    "print(f\"   X_data:        {X_data.shape}\")\n",
    "print(f\"   y_labels:      {y_labels.shape}\")\n",
    "print(f\"   y_categorical: {y_categorical.shape}\")\n",
    "print(f\"\\n‚úÖ Classes: {len(np.unique(y_labels))} (should be 6)\")\n",
    "print(f\"‚úÖ Label range: {y_labels.min()}-{y_labels.max()} (should be 0-5)\")\n",
    "print(f\"‚úÖ Model output: {model.output_shape[-1]} (should be 6)\")\n",
    "\n",
    "assert X_data.shape[0] == y_labels.shape[0] == y_categorical.shape[0], \"Shape mismatch!\"\n",
    "assert len(np.unique(y_labels)) == 6, \"Should have 6 classes!\"\n",
    "assert y_labels.max() == 5, \"Max label should be 5!\"\n",
    "assert model.output_shape[-1] == 6, \"Model should output 6 classes!\"\n",
    "\n",
    "print(\"\\n‚úÖ All checks passed - ready to train!\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 5. K-Fold Cross-Validation Training\n",
    "\n",
    "# %%\n",
    "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "fold_results = []\n",
    "fold_histories = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING K-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_data, y_labels), 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FOLD {fold}/{K_FOLDS}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X_data[train_idx], X_data[val_idx]\n",
    "    y_train, y_val = y_categorical[train_idx], y_categorical[val_idx]\n",
    "    \n",
    "    print(f\"Train: {X_train.shape[0]:,} samples | Val: {X_val.shape[0]:,} samples\")\n",
    "    \n",
    "    # Build fresh model for this fold\n",
    "    fallnet_fold = FallNet(input_shape=(200, 6), n_classes=6)\n",
    "    model_fold = fallnet_fold.build_lmu_only()\n",
    "    model_fold = fallnet_fold.compile_model()\n",
    "    \n",
    "    # Define callbacks for THIS fold\n",
    "    fold_callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=10,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(output_dir / f'fallnet_lmu_boosted_fold_{fold}.keras'),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    # %% [markdown]\n",
    "## 5.5 Calculate Class Weights\n",
    "\n",
    "# %%\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CALCULATING CLASS WEIGHTS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Calculate balanced weights\n",
    "    class_weights_array = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_labels),\n",
    "        y=y_labels\n",
    "    )\n",
    "# 2. Convert to dictionary\n",
    "    class_weights = dict(enumerate(class_weights_array))\n",
    "\n",
    "    # 3. APPLY SURGICAL BOOSTS\n",
    "    # We are manually overriding specific classes to hit 90%\n",
    "    class_weights[0] *= 1.5  # Boost Walking\n",
    "    class_weights[3] *= 3.0  # Triple boost Stumbles (The \"Problem Child\")\n",
    "    class_weights[4] *= 1.2  # Nudge Fall Initiation (Safety first)\n",
    "    \n",
    "    # 4. Cap weights to prevent instability\n",
    "    MAX_WEIGHT = 5.0 \n",
    "    for k in class_weights:\n",
    "        class_weights[k] = min(class_weights[k], MAX_WEIGHT)\n",
    "\n",
    "    print(\"\\nTargeted Class Weights:\")\n",
    "    for cls_idx in range(6):\n",
    "        print(f\"  {reverse_label_map[cls_idx]:<30s}: {class_weights[cls_idx]:.2f}x\")\n",
    "\n",
    "    print(f\"\\nTraining fold {fold}...\")\n",
    "    history = model_fold.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        class_weight=class_weights,  # ‚Üê ADD THIS LINE!\n",
    "        callbacks=fold_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_acc, val_precision, val_recall = model_fold.evaluate(X_val, y_val, batch_size=2, verbose=0)\n",
    "    val_f1 = 2 * (val_precision * val_recall) / (val_precision + val_recall) if (val_precision + val_recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Fold {fold} Results:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Loss:      {val_loss:.4f}\")\n",
    "    print(f\"Accuracy:  {val_acc:.4f}\")\n",
    "    print(f\"Precision: {val_precision:.4f}\")\n",
    "    print(f\"Recall:    {val_recall:.4f}\")\n",
    "    print(f\"F1-Score:  {val_f1:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    fold_results.append({\n",
    "        'fold': fold,\n",
    "        'val_loss': val_loss,\n",
    "        'val_accuracy': val_acc,\n",
    "        'val_precision': val_precision,\n",
    "        'val_recall': val_recall,\n",
    "        'val_f1': val_f1\n",
    "    })\n",
    "    \n",
    "    fold_histories.append(history.history)\n",
    "    \n",
    "    print(f\"‚úÖ Model saved: fallnet_boosted_lmu_fold_{fold}.keras\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"K-FOLD CROSS-VALIDATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# %% [markdown]\n",
    "## 6. Aggregate Results\n",
    "\n",
    "# %%\n",
    "results_df = pd.DataFrame(fold_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS ACROSS ALL FOLDS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AVERAGE PERFORMANCE ¬± STD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "mean_results = results_df.mean(numeric_only=True)\n",
    "std_results = results_df.std(numeric_only=True)\n",
    "\n",
    "metrics_table = []\n",
    "for metric in ['val_loss', 'val_accuracy', 'val_precision', 'val_recall', 'val_f1']:\n",
    "    metrics_table.append({\n",
    "        'Metric': metric,\n",
    "        'Mean': f\"{mean_results[metric]:.4f}\",\n",
    "        'Std': f\"¬±{std_results[metric]:.4f}\"\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_table)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# %% [markdown]\n",
    "## 7. Visualize Training History\n",
    "\n",
    "# %%\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = [\n",
    "    ('loss', 'Loss'),\n",
    "    ('accuracy', 'Accuracy'),\n",
    "    ('precision', 'Precision'),\n",
    "    ('recall', 'Recall')\n",
    "]\n",
    "\n",
    "for idx, (metric, title) in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    for fold, history in enumerate(fold_histories, 1):\n",
    "        epochs = range(1, len(history[metric]) + 1)\n",
    "        ax.plot(epochs, history[metric], label=f'Fold {fold} Train', alpha=0.5, linewidth=1)\n",
    "        ax.plot(epochs, history[f'val_{metric}'], label=f'Fold {fold} Val', \n",
    "                linestyle='--', alpha=0.7, linewidth=1.5)\n",
    "    \n",
    "    ax.set_title(f'{title} Across All Folds', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch', fontsize=11)\n",
    "    ax.set_ylabel(title, fontsize=11)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=7)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('FallNet Training History - 5-Fold Cross-Validation', \n",
    "             fontsize=15, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Training history saved to {output_dir / 'training_history.png'}\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 8. Detailed Evaluation on Best Fold\n",
    "\n",
    "# %%\n",
    "best_fold = int(results_df.loc[results_df['val_f1'].idxmax(), 'fold'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"DETAILED EVALUATION - BEST FOLD #{best_fold}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best fold F1-Score: {results_df.loc[results_df['fold']==best_fold, 'val_f1'].values[0]:.4f}\")\n",
    "\n",
    "# Load best model\n",
    "best_model = keras.models.load_model(output_dir / f'fallnet_boosted_lmu_fold_{fold}.keras')\n",
    "\n",
    "# Get predictions on ALL data\n",
    "y_pred_probs = best_model.predict(X_data, verbose=0)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Classification report\n",
    "class_names = [reverse_label_map[i] for i in range(6)]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSIFICATION REPORT (Best Fold on All Data)\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_labels, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "# %% [markdown]\n",
    "## 9. Per-Class Detailed Metrics\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-CLASS DETAILED METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Class':<40s} {'Precision':<12s} {'Recall':<12s} {'F1-Score':<12s} {'Support'}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "for cls_idx in range(6):\n",
    "    precision = precision_score(y_labels == cls_idx, y_pred == cls_idx, zero_division=0)\n",
    "    recall = recall_score(y_labels == cls_idx, y_pred == cls_idx, zero_division=0)\n",
    "    f1 = f1_score(y_labels == cls_idx, y_pred == cls_idx, zero_division=0)\n",
    "    support = np.sum(y_labels == cls_idx)\n",
    "    \n",
    "    print(f\"{reverse_label_map[cls_idx]:<40s} {precision:<12.4f} {recall:<12.4f} {f1:<12.4f} {support}\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 10. Confusion Matrix\n",
    "\n",
    "# %%\n",
    "cm = confusion_matrix(y_labels, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "plt.title('Confusion Matrix - Best Fold (6 Classes)', fontsize=15, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Confusion matrix saved to {output_dir / 'confusion_matrix.png'}\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 11. Final Summary\n",
    "\n",
    "# %%\n",
    "# Get Fall_Initiation metrics\n",
    "fall_init_idx = label_map[\"Fall_Initiation\"]\n",
    "fall_init_precision = precision_score(y_labels == fall_init_idx, y_pred == fall_init_idx)\n",
    "fall_init_recall = recall_score(y_labels == fall_init_idx, y_pred == fall_init_idx)\n",
    "fall_init_f1 = f1_score(y_labels == fall_init_idx, y_pred == fall_init_idx)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "‚úÖ Successfully trained FallNet with 5-fold cross-validation\n",
    "\n",
    "Configuration:\n",
    "  - Model: CNN-LSTM Ensemble (6 classes)\n",
    "  - Total samples: {len(y_labels):,}\n",
    "  - Training samples per fold: ~{len(y_labels)*0.8//K_FOLDS:,.0f}\n",
    "  - Validation samples per fold: ~{len(y_labels)*0.2//K_FOLDS:,.0f}\n",
    "\n",
    "Average Performance (5-fold CV):\n",
    "  - Accuracy:  {mean_results['val_accuracy']:.4f} ¬± {std_results['val_accuracy']:.4f}\n",
    "  - Precision: {mean_results['val_precision']:.4f} ¬± {std_results['val_precision']:.4f}\n",
    "  - Recall:    {mean_results['val_recall']:.4f} ¬± {std_results['val_recall']:.4f}\n",
    "  - F1-Score:  {mean_results['val_f1']:.4f} ¬± {std_results['val_f1']:.4f}\n",
    "\n",
    "Fall_Initiation Performance (Critical Class):\n",
    "  - Recall (Sensitivity): {fall_init_recall:.4f}\n",
    "  - F1-Score:             {fall_init_f1:.4f}\n",
    "\n",
    "Saved Files:\n",
    "  - Training history:    {output_dir / 'training_history.png'}\n",
    "  - Confusion matrix:    {output_dir / 'confusion_matrix.png'}\n",
    "  - Best model:          {output_dir / f'fallnet_fold_{best_fold}.keras'}\n",
    "  - All fold models:     {output_dir / 'fallnet_fold_*.keras'}\n",
    "\"\"\"\n",
    "uv\n",
    "print(summary)\n",
    "\n",
    "with open(output_dir / 'training_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"‚úÖ Summary saved to {output_dir / 'training_summary.txt'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
