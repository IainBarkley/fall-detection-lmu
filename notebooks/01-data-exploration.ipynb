{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bca2cde-fb8d-4b35-b32b-510499b512f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (119061, 14)\n",
      "\n",
      "Columns: ['Unnamed: 0', 'TimeStamp(s)', 'FrameCounter', 'AccX', 'AccY', 'AccZ', 'GyrX', 'GyrY', 'GyrZ', 'EulerX', 'EulerY', 'EulerZ', 'Fall/No Fall', 'Fall Type']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717/3407601305.py:6: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../fall_detection_data/merged_processed/SA06-merged.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TimeStamp(s)</th>\n",
       "      <th>FrameCounter</th>\n",
       "      <th>AccX</th>\n",
       "      <th>AccY</th>\n",
       "      <th>AccZ</th>\n",
       "      <th>GyrX</th>\n",
       "      <th>GyrY</th>\n",
       "      <th>GyrZ</th>\n",
       "      <th>EulerX</th>\n",
       "      <th>EulerY</th>\n",
       "      <th>EulerZ</th>\n",
       "      <th>Fall/No Fall</th>\n",
       "      <th>Fall Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-1.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.057296</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.343775</td>\n",
       "      <td>87.788625</td>\n",
       "      <td>-2.859060</td>\n",
       "      <td>-20.202499</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-1.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.057296</td>\n",
       "      <td>0.229183</td>\n",
       "      <td>0.229183</td>\n",
       "      <td>87.788625</td>\n",
       "      <td>-2.864790</td>\n",
       "      <td>-20.196769</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-1.009</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.458366</td>\n",
       "      <td>0.286479</td>\n",
       "      <td>0.171887</td>\n",
       "      <td>87.782895</td>\n",
       "      <td>-2.864790</td>\n",
       "      <td>-20.196769</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-1.011</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.916733</td>\n",
       "      <td>0.458366</td>\n",
       "      <td>0.229183</td>\n",
       "      <td>87.771436</td>\n",
       "      <td>-2.870520</td>\n",
       "      <td>-20.191040</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-1.012</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-1.088620</td>\n",
       "      <td>0.343775</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>87.817273</td>\n",
       "      <td>-2.841872</td>\n",
       "      <td>-20.191040</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  TimeStamp(s)  FrameCounter   AccX   AccY   AccZ      GyrX  \\\n",
       "0           0          0.01             1 -0.053 -1.004  0.000 -0.057296   \n",
       "1           1          0.02             2 -0.054 -1.007  0.000 -0.057296   \n",
       "2           2          0.03             3 -0.056 -1.009 -0.004 -0.458366   \n",
       "3           3          0.04             4 -0.053 -1.011 -0.009 -0.916733   \n",
       "4           4          0.05             5 -0.052 -1.012 -0.011 -1.088620   \n",
       "\n",
       "       GyrY      GyrZ     EulerX    EulerY     EulerZ  Fall/No Fall Fall Type  \n",
       "0  0.401071  0.343775  87.788625 -2.859060 -20.202499             0       0.0  \n",
       "1  0.229183  0.229183  87.788625 -2.864790 -20.196769             0       0.0  \n",
       "2  0.286479  0.171887  87.782895 -2.864790 -20.196769             0       0.0  \n",
       "3  0.458366  0.229183  87.771436 -2.870520 -20.191040             0       0.0  \n",
       "4  0.343775  0.401071  87.817273 -2.841872 -20.191040             0       0.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load one subject's data to explore\n",
    "df = pd.read_csv('../fall_detection_data/merged_processed/SA06-merged.csv')\n",
    "\n",
    "# Basic exploration\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1610c945-4ef4-40a3-9b8c-05a22d383953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall vs No Fall distribution:\n",
      "Fall/No Fall\n",
      "0    114082\n",
      "1      4979\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fall Type distribution:\n",
      "Fall Type\n",
      "0.0    65536\n",
      "0      44358\n",
      "0.0     4188\n",
      "T26      475\n",
      "T23      415\n",
      "T20      412\n",
      "T28      398\n",
      "T30      375\n",
      "T25      373\n",
      "T31      347\n",
      "T22      334\n",
      "T29      328\n",
      "T34      318\n",
      "T24      318\n",
      "T32      306\n",
      "T33      233\n",
      "T27      196\n",
      "T21      151\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of fall samples: 4979\n",
      "\n",
      "Sampling interval stats:\n",
      "Mean: 0.0001s\n",
      "Std: 0.318303s\n"
     ]
    }
   ],
   "source": [
    "# Check the data distribution\n",
    "print(\"Fall vs No Fall distribution:\")\n",
    "print(df['Fall/No Fall'].value_counts())\n",
    "\n",
    "print(\"\\nFall Type distribution:\")\n",
    "print(df['Fall Type'].value_counts())\n",
    "\n",
    "# Check if there are any fall events in this file\n",
    "fall_samples = df[df['Fall/No Fall'] == 1]\n",
    "print(f\"\\nNumber of fall samples: {len(fall_samples)}\")\n",
    "\n",
    "# Examine sampling rate consistency\n",
    "time_diffs = df['TimeStamp(s)'].diff().dropna()\n",
    "print(f\"\\nSampling interval stats:\")\n",
    "print(f\"Mean: {time_diffs.mean():.4f}s\")\n",
    "print(f\"Std: {time_diffs.std():.6f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a84bd40-30a1-422c-b6cb-eebe0b69a0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files for SA06: 156\n",
      "First few files: ['S06T01R01.csv', 'S06T02R01.csv', 'S06T02R02.csv', 'S06T02R03.csv', 'S06T02R04.csv', 'S06T02R05.csv', 'S06T03R01.csv', 'S06T03R02.csv', 'S06T03R03.csv', 'S06T03R04.csv']\n",
      "\n",
      "Raw file shape: (3072, 11)\n",
      "Raw file columns: ['TimeStamp(s)', 'FrameCounter', 'AccX', 'AccY', 'AccZ', 'GyrX', 'GyrY', 'GyrZ', 'EulerX', 'EulerY', 'EulerZ']\n",
      "\n",
      "First few rows of raw data:\n",
      "   TimeStamp(s)  FrameCounter   AccX   AccY   AccZ      GyrX      GyrY  \\\n",
      "0          0.01             1 -0.053 -1.004  0.000 -0.057296  0.401071   \n",
      "1          0.02             2 -0.054 -1.007  0.000 -0.057296  0.229183   \n",
      "2          0.03             3 -0.056 -1.009 -0.004 -0.458366  0.286479   \n",
      "3          0.04             4 -0.053 -1.011 -0.009 -0.916733  0.458366   \n",
      "4          0.05             5 -0.052 -1.012 -0.011 -1.088620  0.343775   \n",
      "\n",
      "       GyrZ     EulerX    EulerY     EulerZ  \n",
      "0  0.343775  87.788625 -2.859060 -20.202499  \n",
      "1  0.229183  87.788625 -2.864790 -20.196769  \n",
      "2  0.171887  87.782895 -2.864790 -20.196769  \n",
      "3  0.229183  87.771436 -2.870520 -20.191040  \n",
      "4  0.401071  87.817273 -2.841872 -20.191040  \n",
      "\n",
      "All unique task codes:\n",
      "['F01 (20)' 'F02 (21)' 'F03 (22)' 'F04 (23)' 'F05 (24)' 'F06 (25)'\n",
      " 'F07 (26)' 'F08 (27)' 'F09 (28)' 'F10 (29)' 'F11 (30)' 'F12 (31)'\n",
      " 'F13 (32)' 'F14 (33)' 'F15 (34)']\n",
      "\n",
      "Task descriptions:\n",
      "F01 (20): Forward fall when trying to sit down\n",
      "F02 (21): Backward fall when trying to sit down\n",
      "F03 (22): lateral fall when trying to sit down\n",
      "F04 (23): Forward fall when trying to get up\n",
      "F05 (24): lateral fall when trying to get up\n",
      "F06 (25): Forward fall while sitting, caused by fainting\n",
      "F07 (26): lateral fall while sitting, caused by fainting\n",
      "F08 (27): Backward fall while sitting, caused by fainting\n",
      "F09 (28): Vertical(forward) fall while walking caused by fainting\n",
      "F10 (29): Fall while walking, with use of hands in a table to dampen fall, caused by fainting\n",
      "F11 (30): Forward fall while walking caused by a trip\n",
      "F12 (31): Forward fall while jogging caused by a trip\n",
      "F13 (32): Forward fall while walking caused by a slip\n",
      "F14 (33): Forward lateral fall while walking caused by a slip\n",
      "F15 (34): Backward fall while walking caused by a slip\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# First, load the label file (this creates df_labels)\n",
    "label_file = '../fall_detection_data/label_data/SA06_label.xlsx'\n",
    "df_labels = pd.read_excel(label_file)\n",
    "\n",
    "# Now check the sensor data structure\n",
    "sensor_data_path = '../fall_detection_data/sensor_data/SA06'\n",
    "files = os.listdir(sensor_data_path)\n",
    "print(f\"Number of files for SA06: {len(files)}\")\n",
    "print(\"First few files:\", files[:10])\n",
    "\n",
    "# Load one individual sensor file\n",
    "sample_file = os.path.join(sensor_data_path, files[0])\n",
    "df_raw = pd.read_csv(sample_file)\n",
    "print(f\"\\nRaw file shape: {df_raw.shape}\")\n",
    "print(\"Raw file columns:\", df_raw.columns.tolist())\n",
    "print(\"\\nFirst few rows of raw data:\")\n",
    "print(df_raw.head())\n",
    "\n",
    "# Now df_labels is defined, so this will work\n",
    "print(\"\\nAll unique task codes:\")\n",
    "print(df_labels['Task Code (Task ID)'].dropna().unique())\n",
    "\n",
    "print(\"\\nTask descriptions:\")\n",
    "for idx, row in df_labels.dropna(subset=['Task Code (Task ID)']).iterrows():\n",
    "    print(f\"{row['Task Code (Task ID)']}: {row['Description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1066984c-69e3-45bb-bc21-a57722509ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files for SA06: 156\n",
      "First few files: ['S06T01R01.csv', 'S06T02R01.csv', 'S06T02R02.csv', 'S06T02R03.csv', 'S06T02R04.csv', 'S06T02R05.csv', 'S06T03R01.csv', 'S06T03R02.csv', 'S06T03R03.csv', 'S06T03R04.csv']\n",
      "\n",
      "Raw file shape: (3072, 11)\n",
      "Raw file columns: ['TimeStamp(s)', 'FrameCounter', 'AccX', 'AccY', 'AccZ', 'GyrX', 'GyrY', 'GyrZ', 'EulerX', 'EulerY', 'EulerZ']\n",
      "\n",
      "First few rows of raw data:\n",
      "All unique task codes:\n",
      "['F01 (20)' 'F02 (21)' 'F03 (22)' 'F04 (23)' 'F05 (24)' 'F06 (25)'\n",
      " 'F07 (26)' 'F08 (27)' 'F09 (28)' 'F10 (29)' 'F11 (30)' 'F12 (31)'\n",
      " 'F13 (32)' 'F14 (33)' 'F15 (34)']\n",
      "\n",
      "Task descriptions:\n",
      "F01 (20): Forward fall when trying to sit down\n",
      "F02 (21): Backward fall when trying to sit down\n",
      "F03 (22): lateral fall when trying to sit down\n",
      "F04 (23): Forward fall when trying to get up\n",
      "F05 (24): lateral fall when trying to get up\n",
      "F06 (25): Forward fall while sitting, caused by fainting\n",
      "F07 (26): lateral fall while sitting, caused by fainting\n",
      "F08 (27): Backward fall while sitting, caused by fainting\n",
      "F09 (28): Vertical(forward) fall while walking caused by fainting\n",
      "F10 (29): Fall while walking, with use of hands in a table to dampen fall, caused by fainting\n",
      "F11 (30): Forward fall while walking caused by a trip\n",
      "F12 (31): Forward fall while jogging caused by a trip\n",
      "F13 (32): Forward fall while walking caused by a slip\n",
      "F14 (33): Forward lateral fall while walking caused by a slip\n",
      "F15 (34): Backward fall while walking caused by a slip\n"
     ]
    }
   ],
   "source": [
    "# Check what individual files look like\n",
    "import os\n",
    "\n",
    "# Look at the directory structure\n",
    "sensor_data_path = '../fall_detection_data/sensor_data/SA06'\n",
    "files = os.listdir(sensor_data_path)\n",
    "print(f\"Number of files for SA06: {len(files)}\")\n",
    "print(\"First few files:\", files[:10])\n",
    "\n",
    "# Load one individual sensor file\n",
    "sample_file = os.path.join(sensor_data_path, files[0])\n",
    "df_raw = pd.read_csv(sample_file)\n",
    "\n",
    "print(f\"\\nRaw file shape: {df_raw.shape}\")\n",
    "print(\"Raw file columns:\", df_raw.columns.tolist())\n",
    "print(\"\\nFirst few rows of raw data:\")\n",
    "df_raw.head()\n",
    "\n",
    "# Check what task codes exist in the full label file\n",
    "print(\"All unique task codes:\")\n",
    "print(df_labels['Task Code (Task ID)'].dropna().unique())\n",
    "\n",
    "print(\"\\nTask descriptions:\")\n",
    "for idx, row in df_labels.dropna(subset=['Task Code (Task ID)']).iterrows():\n",
    "    print(f\"{row['Task Code (Task ID)']}: {row['Description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc573539-5df0-4357-87c4-a49cd5a787d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label file shape: (71, 5)\n",
      "Label file columns: ['Task Code (Task ID)', 'Description', 'Trial ID', 'Fall_onset_frame', 'Fall_impact_frame']\n",
      "\n",
      "Label file preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task Code (Task ID)</th>\n",
       "      <th>Description</th>\n",
       "      <th>Trial ID</th>\n",
       "      <th>Fall_onset_frame</th>\n",
       "      <th>Fall_impact_frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F01 (20)</td>\n",
       "      <td>Forward fall when trying to sit down</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>184</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>184</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>151</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Task Code (Task ID)                           Description  Trial ID  \\\n",
       "0            F01 (20)  Forward fall when trying to sit down         1   \n",
       "1                 NaN                                   NaN         2   \n",
       "2                 NaN                                   NaN         3   \n",
       "3                 NaN                                   NaN         4   \n",
       "4                 NaN                                   NaN         5   \n",
       "\n",
       "   Fall_onset_frame  Fall_impact_frame  \n",
       "0               130                208  \n",
       "1               184                272  \n",
       "2               184                260  \n",
       "3               151                231  \n",
       "4               128                223  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the label file\n",
    "label_file = '../fall_detection_data/label_data/SA06_label.xlsx'\n",
    "df_labels = pd.read_excel(label_file)\n",
    "\n",
    "print(f\"Label file shape: {df_labels.shape}\")\n",
    "print(\"Label file columns:\", df_labels.columns.tolist())\n",
    "print(\"\\nLabel file preview:\")\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f23b899-dc1a-47d5-a7af-eae0c085e120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All task numbers in sensor files: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n",
      "Task range: 1 to 36\n",
      "Excel sheets available: ['Sheet1']\n",
      "\n",
      "Sheet 'Sheet1':\n",
      "  Shape: (71, 5)\n",
      "  Columns: ['Task Code (Task ID)', 'Description', 'Trial ID', 'Fall_onset_frame', 'Fall_impact_frame']\n",
      "  Sample data: {'Task Code (Task ID)': 'F01 (20)', 'Description': 'Forward fall when trying to sit down', 'Trial ID': 1, 'Fall_onset_frame': 130, 'Fall_impact_frame': 208}\n"
     ]
    }
   ],
   "source": [
    "# Check ALL task numbers in your sensor data\n",
    "task_numbers = [int(f.split('T')[1].split('R')[0]) for f in files if 'T' in f]\n",
    "all_tasks = sorted(set(task_numbers))\n",
    "print(f\"All task numbers in sensor files: {all_tasks}\")\n",
    "print(f\"Task range: {min(all_tasks)} to {max(all_tasks)}\")\n",
    "\n",
    "# Check if there are multiple sheets in the Excel file\n",
    "excel_file = pd.ExcelFile('../fall_detection_data/label_data/SA06_label.xlsx')\n",
    "print(f\"Excel sheets available: {excel_file.sheet_names}\")\n",
    "\n",
    "# If there are multiple sheets, check each one\n",
    "for sheet in excel_file.sheet_names:\n",
    "    df_sheet = pd.read_excel(excel_file, sheet_name=sheet)\n",
    "    print(f\"\\nSheet '{sheet}':\")\n",
    "    print(f\"  Shape: {df_sheet.shape}\")\n",
    "    print(f\"  Columns: {df_sheet.columns.tolist()}\")\n",
    "    if len(df_sheet) > 0:\n",
    "        print(f\"  Sample data: {df_sheet.iloc[0].to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a9b2b97-d6fc-4fe9-bc5a-8d49d3487f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T01 file: S06T01R01.csv\n",
      "T01 shape: (3072, 11)\n",
      "T01 data preview:\n",
      "   TimeStamp(s)  FrameCounter   AccX   AccY   AccZ      GyrX      GyrY  \\\n",
      "0          0.01             1 -0.053 -1.004  0.000 -0.057296  0.401071   \n",
      "1          0.02             2 -0.054 -1.007  0.000 -0.057296  0.229183   \n",
      "2          0.03             3 -0.056 -1.009 -0.004 -0.458366  0.286479   \n",
      "3          0.04             4 -0.053 -1.011 -0.009 -0.916733  0.458366   \n",
      "4          0.05             5 -0.052 -1.012 -0.011 -1.088620  0.343775   \n",
      "\n",
      "       GyrZ     EulerX    EulerY     EulerZ  \n",
      "0  0.343775  87.788625 -2.859060 -20.202499  \n",
      "1  0.229183  87.788625 -2.864790 -20.196769  \n",
      "2  0.171887  87.782895 -2.864790 -20.196769  \n",
      "3  0.229183  87.771436 -2.870520 -20.191040  \n",
      "4  0.401071  87.817273 -2.841872 -20.191040  \n",
      "\n",
      "T20 file: S06T20R01.csv\n",
      "T20 shape: (536, 11)\n",
      "T20 data preview:\n",
      "   TimeStamp(s)  FrameCounter   AccX   AccY   AccZ       GyrX       GyrY  \\\n",
      "0          0.01             1 -0.250 -0.809 -0.188 -35.122325  14.954204   \n",
      "1          0.02             2 -0.070 -1.023  0.056  -2.578311  -0.171887   \n",
      "2          0.03             3 -0.070 -1.019  0.056  -3.781523   0.859437   \n",
      "3          0.04             4 -0.070 -1.019  0.044  -5.385805   1.948057   \n",
      "4          0.05             5 -0.069 -1.025  0.028  -6.531721   2.578311   \n",
      "\n",
      "       GyrZ     EulerX     EulerY     EulerZ  \n",
      "0  4.755551  40.657100 -14.782316  56.952025  \n",
      "1 -0.114592  40.628452 -14.782316  56.946296  \n",
      "2 -0.286479  40.588345 -14.776587  56.952025  \n",
      "3 -0.458366  40.531049 -14.759398  56.957755  \n",
      "4 -0.687550  41.310272 -14.656266  56.539495  \n"
     ]
    }
   ],
   "source": [
    "# Let's check what a non-fall task looks like\n",
    "# Load a T01 file to see if it has different characteristics\n",
    "t01_file = [f for f in files if 'T01' in f][0]\n",
    "t01_path = os.path.join(sensor_data_path, t01_file)\n",
    "df_t01 = pd.read_csv(t01_path)\n",
    "\n",
    "print(f\"T01 file: {t01_file}\")\n",
    "print(f\"T01 shape: {df_t01.shape}\")\n",
    "print(\"T01 data preview:\")\n",
    "print(df_t01.head())\n",
    "\n",
    "# Compare with a fall task\n",
    "t20_file = [f for f in files if 'T20' in f][0]\n",
    "t20_path = os.path.join(sensor_data_path, t20_file)\n",
    "df_t20 = pd.read_csv(t20_path)\n",
    "\n",
    "print(f\"\\nT20 file: {t20_file}\")\n",
    "print(f\"T20 shape: {df_t20.shape}\")\n",
    "print(\"T20 data preview:\")\n",
    "print(df_t20.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b9ef802-9903-4bb0-acfb-3cc5600a574c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T02: 729 frames (~7.3s)\n",
      "T05: 764 frames (~7.6s)\n",
      "T10: 802 frames (~8.0s)\n",
      "T15: 320 frames (~3.2s)\n",
      "T19: 643 frames (~6.4s)\n"
     ]
    }
   ],
   "source": [
    "# Check a few more ADL tasks to see if they have different characteristics\n",
    "for task in [2, 5, 10, 15, 19]:\n",
    "    task_file = [f for f in files if f'T{task:02d}' in f][0]\n",
    "    task_path = os.path.join(sensor_data_path, task_file)\n",
    "    df_task = pd.read_csv(task_path)\n",
    "    print(f\"T{task:02d}: {len(df_task)} frames (~{len(df_task)/100:.1f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9993b4b1-19c8-4fa1-a7ba-58c2549aafd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single task analysis:\n",
      "Task 1 samples: 3072\n",
      "Time span: 30.72 seconds\n",
      "Expected duration at 100Hz: 30.72 seconds\n",
      "\n",
      "Timestamp investigation:\n",
      "Task 1: 0.0100 - 30.7200\n",
      "Task 2: 0.0100 - 7.5800\n",
      "Task 3: 0.0100 - 6.4700\n"
     ]
    }
   ],
   "source": [
    "# Check actual file structure\n",
    "sample_subject = combined_df[combined_df['Subject'] == 'SA06']\n",
    "sample_task = sample_subject[sample_subject['Task'] == 1]\n",
    "\n",
    "print(\"Single task analysis:\")\n",
    "print(f\"Task 1 samples: {len(sample_task)}\")\n",
    "print(f\"Time span: {sample_task['TimeStamp(s)'].max():.2f} seconds\")\n",
    "print(f\"Expected duration at 100Hz: {len(sample_task)/100:.2f} seconds\")\n",
    "\n",
    "# Check if timestamps are cumulative across tasks\n",
    "print(\"\\nTimestamp investigation:\")\n",
    "for task in [1, 2, 3]:\n",
    "    task_data = sample_subject[sample_subject['Task'] == task]\n",
    "    print(f\"Task {task}: {task_data['TimeStamp(s)'].min():.4f} - {task_data['TimeStamp(s)'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6166f0e-30fc-4ef7-b8cb-44f2c68e5d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task duration summary:\n",
      "Duration\n",
      "2.88          [4]\n",
      "3.20         [15]\n",
      "3.57         [27]\n",
      "4.43     [14, 26]\n",
      "4.47         [22]\n",
      "4.53         [23]\n",
      "4.70         [21]\n",
      "4.79         [25]\n",
      "4.96          [3]\n",
      "5.36         [20]\n",
      "5.50         [24]\n",
      "5.83         [31]\n",
      "6.02         [30]\n",
      "6.19         [13]\n",
      "6.37         [33]\n",
      "6.43         [19]\n",
      "6.51         [32]\n",
      "6.77         [28]\n",
      "7.29          [2]\n",
      "7.35         [16]\n",
      "7.44         [34]\n",
      "7.64          [5]\n",
      "7.88         [29]\n",
      "8.02     [10, 36]\n",
      "8.25          [9]\n",
      "8.82         [18]\n",
      "9.04          [7]\n",
      "9.31          [8]\n",
      "12.14         [6]\n",
      "12.25        [35]\n",
      "30.25        [12]\n",
      "30.72         [1]\n",
      "31.47        [17]\n",
      "32.02        [11]\n",
      "Name: Task, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check task duration patterns across your dataset\n",
    "task_durations = []\n",
    "for task_num in range(1, 37):\n",
    "    task_files = [f for f in files if f'T{task_num:02d}' in f]\n",
    "    if task_files:\n",
    "        sample_file = os.path.join(sensor_data_path, task_files[0])\n",
    "        df_sample = pd.read_csv(sample_file)\n",
    "        duration = df_sample['TimeStamp(s)'].max()\n",
    "        task_durations.append({'Task': task_num, 'Duration': duration, 'Samples': len(df_sample)})\n",
    "\n",
    "duration_df = pd.DataFrame(task_durations)\n",
    "print(\"Task duration summary:\")\n",
    "print(duration_df.groupby('Duration')['Task'].apply(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d162016-215c-4718-9a9a-3bbd65ce3b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_tasks_by_type():\n",
    "    # Static tasks - extract 20s segments\n",
    "    static_tasks = [1, 11, 12, 17]\n",
    "    \n",
    "    # ADL tasks - use full duration\n",
    "    adl_tasks = list(range(2, 20))  # T02-T19\n",
    "    \n",
    "    # Fall tasks - apply temporal segmentation\n",
    "    fall_tasks = list(range(20, 35))  # T20-T34\n",
    "    \n",
    "    return static_tasks, adl_tasks, fall_tasks\n",
    "\n",
    "def process_task_by_category(task_num, duration):\n",
    "    if duration > 25:  # Static tasks\n",
    "        return \"extract_20s_segment\"\n",
    "    elif 20 <= task_num <= 34:  # Fall tasks  \n",
    "        return \"apply_temporal_segmentation\"\n",
    "    else:  # ADL tasks\n",
    "        return \"use_full_trial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37742da6-5315-4efc-a1e4-320de2d8aba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data shape: (119061, 18)\n",
      "Fall trials: 29124\n",
      "Tasks: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21), np.int64(22), np.int64(23), np.int64(24), np.int64(25), np.int64(26), np.int64(27), np.int64(28), np.int64(29), np.int64(30), np.int64(31), np.int64(32), np.int64(33), np.int64(34), np.int64(35), np.int64(36)]\n"
     ]
    }
   ],
   "source": [
    "def merge_sensor_with_labels(subject_id):\n",
    "    \"\"\"\n",
    "    Merge sensor files with their corresponding temporal labels\n",
    "    \"\"\"\n",
    "    # Load labels\n",
    "    labels_df = pd.read_excel(f'{label_path}/{subject_id}_label.xlsx')\n",
    "    \n",
    "    # Load all sensor files for this subject\n",
    "    subject_folder = f'{sensor_path}/{subject_id}'\n",
    "    files = [f for f in os.listdir(subject_folder) if f.endswith('.csv')]\n",
    "    \n",
    "    merged_trials = []\n",
    "    \n",
    "    for file in files:\n",
    "        # Parse filename to get task and trial\n",
    "        parts = file.replace('.csv', '').split('T')\n",
    "        task_num = int(parts[1].split('R')[0])\n",
    "        trial_num = int(parts[1].split('R')[1])\n",
    "        \n",
    "        # Load sensor data\n",
    "        sensor_df = pd.read_csv(f'{subject_folder}/{file}')\n",
    "        \n",
    "        # Add metadata\n",
    "        sensor_df['Subject'] = subject_id\n",
    "        sensor_df['Task'] = task_num\n",
    "        sensor_df['Trial'] = trial_num\n",
    "        \n",
    "        # Find matching label (for fall tasks only)\n",
    "        matching_label = labels_df[\n",
    "            (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) & \n",
    "            (labels_df['Trial ID'] == trial_num)\n",
    "        ]\n",
    "        \n",
    "        if len(matching_label) > 0:\n",
    "            # Fall task - add temporal labels\n",
    "            sensor_df['Fall_onset_frame'] = matching_label.iloc[0]['Fall_onset_frame']\n",
    "            sensor_df['Fall_impact_frame'] = matching_label.iloc[0]['Fall_impact_frame']\n",
    "            sensor_df['Task_description'] = matching_label.iloc[0]['Description']\n",
    "            sensor_df['Is_Fall'] = True\n",
    "        else:\n",
    "            # ADL task - no fall labels\n",
    "            sensor_df['Fall_onset_frame'] = None\n",
    "            sensor_df['Fall_impact_frame'] = None  \n",
    "            sensor_df['Task_description'] = f'ADL_Task_{task_num}'\n",
    "            sensor_df['Is_Fall'] = False\n",
    "            \n",
    "        merged_trials.append(sensor_df)\n",
    "    \n",
    "    return pd.concat(merged_trials, ignore_index=True)\n",
    "\n",
    "# Test the merging first\n",
    "test_subject = 'SA06'\n",
    "merged_subject_data = merge_sensor_with_labels(test_subject)\n",
    "print(f\"Merged data shape: {merged_subject_data.shape}\")\n",
    "print(f\"Fall trials: {merged_subject_data['Is_Fall'].sum()}\")\n",
    "print(f\"Tasks: {sorted(merged_subject_data['Task'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e927651f-d456-4e6f-9687-4e140f2cc087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n",
      "/tmp/ipykernel_3717/1753526860.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  (labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')) &\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Structure ===\n",
      "Shape: (119061, 18)\n",
      "Columns: ['TimeStamp(s)', 'FrameCounter', 'AccX', 'AccY', 'AccZ', 'GyrX', 'GyrY', 'GyrZ', 'EulerX', 'EulerY', 'EulerZ', 'Subject', 'Task', 'Trial', 'Fall_onset_frame', 'Fall_impact_frame', 'Task_description', 'Is_Fall']\n",
      "Memory usage: 31.8 MB\n",
      "\n",
      "=== Data Completeness ===\n",
      "Missing values per column:\n",
      "TimeStamp(s)             0\n",
      "FrameCounter             0\n",
      "AccX                     0\n",
      "AccY                     0\n",
      "AccZ                     0\n",
      "GyrX                     0\n",
      "GyrY                     0\n",
      "GyrZ                     0\n",
      "EulerX                   0\n",
      "EulerY                   0\n",
      "EulerZ                   0\n",
      "Subject                  0\n",
      "Task                     0\n",
      "Trial                    0\n",
      "Fall_onset_frame     89937\n",
      "Fall_impact_frame    89937\n",
      "Task_description         0\n",
      "Is_Fall                  0\n",
      "dtype: int64\n",
      "\n",
      "=== Fall Labels ===\n",
      "Fall trials: 33\n",
      "Fall samples with onset labels: 29124\n",
      "Fall samples with impact labels: 29124\n",
      "\n",
      "=== Task Distribution ===\n",
      "    Task  Is_Fall  Samples\n",
      "0      1     True     3072\n",
      "1      2    False     2150\n",
      "2      2     True     1477\n",
      "3      3    False     2338\n",
      "4      3     True      496\n",
      "..   ...      ...      ...\n",
      "58    33     True      637\n",
      "59    34    False     2999\n",
      "60    34     True      744\n",
      "61    35    False     3898\n",
      "62    36    False     4301\n",
      "\n",
      "[63 rows x 3 columns]\n",
      "\n",
      "=== Sampling Rate Check ===\n",
      "Sampling interval stats: count    1.560000e+02\n",
      "mean     1.000000e-02\n",
      "std      1.689363e-18\n",
      "min      1.000000e-02\n",
      "25%      1.000000e-02\n",
      "50%      1.000000e-02\n",
      "75%      1.000000e-02\n",
      "max      1.000000e-02\n",
      "Name: TimeStamp(s), dtype: float64\n",
      "\n",
      "=== Fall Example ===\n",
      "Task 20, Trial 1:\n",
      "  Total frames: 536\n",
      "  Fall onset: frame 130 (1.30s)\n",
      "  Fall impact: frame 208 (2.08s)\n",
      "  Fall duration: 0.78s\n"
     ]
    }
   ],
   "source": [
    "# Test merge on one subject first\n",
    "test_subject = 'SA06'\n",
    "merged_data = merge_sensor_with_labels(test_subject)\n",
    "\n",
    "# Basic structure checks\n",
    "print(\"=== Data Structure ===\")\n",
    "print(f\"Shape: {merged_data.shape}\")\n",
    "print(f\"Columns: {merged_data.columns.tolist()}\")\n",
    "print(f\"Memory usage: {merged_data.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Check data completeness\n",
    "print(\"\\n=== Data Completeness ===\")\n",
    "print(f\"Missing values per column:\")\n",
    "print(merged_data.isnull().sum())\n",
    "\n",
    "# Verify temporal labels are correctly attached\n",
    "print(\"\\n=== Fall Labels ===\")\n",
    "fall_data = merged_data[merged_data['Is_Fall'] == True]\n",
    "print(f\"Fall trials: {len(fall_data.groupby(['Task', 'Trial']))}\")\n",
    "print(f\"Fall samples with onset labels: {fall_data['Fall_onset_frame'].notna().sum()}\")\n",
    "print(f\"Fall samples with impact labels: {fall_data['Fall_impact_frame'].notna().sum()}\")\n",
    "\n",
    "# Check task distribution\n",
    "print(\"\\n=== Task Distribution ===\")\n",
    "task_summary = merged_data.groupby(['Task', 'Is_Fall']).size().reset_index(name='Samples')\n",
    "print(task_summary)\n",
    "\n",
    "# Verify sampling rate consistency\n",
    "print(\"\\n=== Sampling Rate Check ===\")\n",
    "sampling_intervals = merged_data.groupby(['Task', 'Trial'])['TimeStamp(s)'].apply(\n",
    "    lambda x: x.diff().dropna().mean()\n",
    ").describe()\n",
    "print(f\"Sampling interval stats: {sampling_intervals}\")\n",
    "\n",
    "# Check one fall example in detail\n",
    "print(\"\\n=== Fall Example ===\")\n",
    "example_fall = merged_data[(merged_data['Task'] == 20) & (merged_data['Trial'] == 1)]\n",
    "if len(example_fall) > 0:\n",
    "    onset = example_fall['Fall_onset_frame'].iloc[0]\n",
    "    impact = example_fall['Fall_impact_frame'].iloc[0]\n",
    "    print(f\"Task 20, Trial 1:\")\n",
    "    print(f\"  Total frames: {len(example_fall)}\")\n",
    "    print(f\"  Fall onset: frame {onset} ({onset*0.01:.2f}s)\")\n",
    "    print(f\"  Fall impact: frame {impact} ({impact*0.01:.2f}s)\")\n",
    "    print(f\"  Fall duration: {(impact-onset)*0.01:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "795ad612-6b1b-482b-86e5-c8e9cfe949df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks with mixed Fall/Non-Fall labels: [2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "\n",
      "Label file task codes:\n",
      "['F01 (20)' nan 'F02 (21)' 'F03 (22)' 'F04 (23)' 'F05 (24)' 'F06 (25)'\n",
      " 'F07 (26)' 'F08 (27)' 'F09 (28)' 'F10 (29)' 'F11 (30)' 'F12 (31)'\n",
      " 'F13 (32)' 'F14 (33)' 'F15 (34)']\n",
      "\n",
      "Matching logic check:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717/1078934777.py:15: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  matching = labels_df[labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot mask with non-boolean array containing NA / NaN values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMatching logic check:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task_num \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m20\u001b[39m, \u001b[32m34\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     matching = \u001b[43mlabels_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabels_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTask Code (Task ID)\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontains\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m(\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtask_num\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTask \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(matching)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m matches in label file\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(matching) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/summerschool2023/projects/fall-detection/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4097\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4094\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.where(key)\n\u001b[32m   4096\u001b[39m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4097\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_bool_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   4098\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_bool_array(key)\n\u001b[32m   4100\u001b[39m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[32m   4101\u001b[39m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/summerschool2023/projects/fall-detection/.venv/lib/python3.13/site-packages/pandas/core/common.py:136\u001b[39m, in \u001b[36mis_bool_indexer\u001b[39m\u001b[34m(key)\u001b[39m\n\u001b[32m    132\u001b[39m     na_msg = \u001b[33m\"\u001b[39m\u001b[33mCannot mask with non-boolean array containing NA / NaN values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lib.is_bool_array(key_array, skipna=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    134\u001b[39m         \u001b[38;5;66;03m# Don't raise on e.g. [\"A\", \"B\", np.nan], see\u001b[39;00m\n\u001b[32m    135\u001b[39m         \u001b[38;5;66;03m#  test_loc_getitem_list_of_labels_categoricalindex_with_na\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(na_msg)\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Cannot mask with non-boolean array containing NA / NaN values"
     ]
    }
   ],
   "source": [
    "# Check which tasks are being labeled as falls incorrectly\n",
    "problem_tasks = merged_data.groupby(['Task', 'Is_Fall']).size().reset_index(name='Count')\n",
    "mixed_tasks = problem_tasks.groupby('Task').size()\n",
    "mixed_tasks = mixed_tasks[mixed_tasks > 1].index.tolist()\n",
    "print(f\"Tasks with mixed Fall/Non-Fall labels: {mixed_tasks}\")\n",
    "\n",
    "# Check the label file directly\n",
    "labels_df = pd.read_excel(f'{label_path}/SA06_label.xlsx')\n",
    "print(\"\\nLabel file task codes:\")\n",
    "print(labels_df['Task Code (Task ID)'].unique())\n",
    "\n",
    "# Check how the matching is working\n",
    "print(\"\\nMatching logic check:\")\n",
    "for task_num in [2, 3, 20, 34]:\n",
    "    matching = labels_df[labels_df['Task Code (Task ID)'].str.contains(f'({task_num})')]\n",
    "    print(f\"Task {task_num}: {len(matching)} matches in label file\")\n",
    "    if len(matching) > 0:\n",
    "        print(f\"  Codes: {matching['Task Code (Task ID)'].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff572b93-5146-4a99-86fa-0cf3c52f8bf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ']' (4086560514.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m]def merge_sensor_with_labels_fixed(subject_id):\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unmatched ']'\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42691aaf-30ad-4179-b146-de058d8b25bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n"
     ]
    }
   ],
   "source": [
    "def merge_sensor_with_labels_fixed(subject_id):\n",
    "    \"\"\"Fixed version with correct label matching\"\"\"\n",
    "    labels_df = pd.read_excel(f'{label_path}/{subject_id}_label.xlsx')\n",
    "    \n",
    "    # Create proper task number mapping\n",
    "    task_mapping = {}\n",
    "    for _, row in labels_df.iterrows():\n",
    "        if pd.notna(row['Task Code (Task ID)']):\n",
    "            # Extract number from \"F01 (20)\" format\n",
    "            task_code = row['Task Code (Task ID)']\n",
    "            if '(' in task_code and ')' in task_code:\n",
    "                task_num = int(task_code.split('(')[1].split(')')[0])\n",
    "                if task_num not in task_mapping:\n",
    "                    task_mapping[task_num] = []\n",
    "                task_mapping[task_num].append(row)\n",
    "    \n",
    "    print(f\"Task mapping: {list(task_mapping.keys())}\")\n",
    "    \n",
    "    # Load sensor files\n",
    "    subject_folder = f'{sensor_path}/{subject_id}'\n",
    "    files = [f for f in os.listdir(subject_folder) if f.endswith('.csv')]\n",
    "    \n",
    "    merged_trials = []\n",
    "    \n",
    "    for file in files:\n",
    "        # Parse filename\n",
    "        parts = file.replace('.csv', '').split('T')\n",
    "        task_num = int(parts[1].split('R')[0])\n",
    "        trial_num = int(parts[1].split('R')[1])\n",
    "        \n",
    "        # Load sensor data\n",
    "        sensor_df = pd.read_csv(f'{subject_folder}/{file}')\n",
    "        sensor_df['Subject'] = subject_id\n",
    "        sensor_df['Task'] = task_num\n",
    "        sensor_df['Trial'] = trial_num\n",
    "        \n",
    "        # Check if this task has fall labels\n",
    "        if task_num in task_mapping:\n",
    "            # This is a fall task\n",
    "            matching_trials = [row for row in task_mapping[task_num] if row['Trial ID'] == trial_num]\n",
    "            if matching_trials:\n",
    "                trial_info = matching_trials[0]\n",
    "                sensor_df['Fall_onset_frame'] = trial_info['Fall_onset_frame']\n",
    "                sensor_df['Fall_impact_frame'] = trial_info['Fall_impact_frame']\n",
    "                sensor_df['Task_description'] = trial_info['Description']\n",
    "                sensor_df['Is_Fall'] = True\n",
    "            else:\n",
    "                # Fall task but no label for this trial\n",
    "                sensor_df['Fall_onset_frame'] = None\n",
    "                sensor_df['Fall_impact_frame'] = None\n",
    "                sensor_df['Task_description'] = f'Fall_Task_{task_num}_No_Label'\n",
    "                sensor_df['Is_Fall'] = True\n",
    "        else:\n",
    "            # ADL task\n",
    "            sensor_df['Fall_onset_frame'] = None\n",
    "            sensor_df['Fall_impact_frame'] = None\n",
    "            sensor_df['Task_description'] = f'ADL_Task_{task_num}'\n",
    "            sensor_df['Is_Fall'] = False\n",
    "            \n",
    "        merged_trials.append(sensor_df)\n",
    "    \n",
    "    return pd.concat(merged_trials, ignore_index=True)\n",
    "\n",
    "# Test the fixed version\n",
    "merged_data_fixed = merge_sensor_with_labels_fixed('SA06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a3eae1e-0539-4643-9036-1f84d5c46066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fixed Data Quality Check ===\n",
      "Task distribution (should be clean now):\n",
      "    Task  Is_Fall  Samples\n",
      "0      1    False     3072\n",
      "1      2    False     3627\n",
      "2      3    False     2834\n",
      "3      4    False     1703\n",
      "4      5    False     4406\n",
      "5      6    False     6641\n",
      "6      7    False     5212\n",
      "7      8    False     4042\n",
      "8      9    False     3366\n",
      "9     10    False     4195\n",
      "10    11    False     3202\n",
      "11    12    False     3025\n",
      "12    13    False     3678\n",
      "13    14    False     3040\n",
      "14    15    False     2183\n",
      "15    16    False     4070\n",
      "16    17    False     3147\n",
      "17    18    False     4464\n",
      "18    19    False     3817\n",
      "19    20     True     2573\n",
      "20    21     True     1847\n",
      "21    22     True     2565\n",
      "22    23     True     2737\n",
      "23    24     True     2732\n",
      "24    25     True     1822\n",
      "25    26     True     2548\n",
      "26    27     True     2072\n",
      "27    28     True     3443\n",
      "28    29     True     3195\n",
      "29    30     True     3150\n",
      "30    31     True     3005\n",
      "31    32     True     3240\n",
      "32    33     True     2466\n",
      "33    34     True     3743\n",
      "34    35    False     3898\n",
      "35    36    False     4301\n",
      "\n",
      "Tasks with mixed labels (should be empty): []\n",
      "\n",
      "Fall task verification:\n",
      "Fall task range: 20-34\n",
      "Fall trials with labels: 8417/41138\n",
      "\n",
      "ADL task verification:\n",
      "ADL task range: 1-36\n",
      "ADL tasks (should have no fall labels): 0\n"
     ]
    }
   ],
   "source": [
    "# Check the fixed data quality\n",
    "print(\"=== Fixed Data Quality Check ===\")\n",
    "task_summary_fixed = merged_data_fixed.groupby(['Task', 'Is_Fall']).size().reset_index(name='Samples')\n",
    "print(\"Task distribution (should be clean now):\")\n",
    "print(task_summary_fixed)\n",
    "\n",
    "# Check for mixed labels (should be empty)\n",
    "mixed_tasks_fixed = task_summary_fixed.groupby('Task').size()\n",
    "mixed_tasks_fixed = mixed_tasks_fixed[mixed_tasks_fixed > 1].index.tolist()\n",
    "print(f\"\\nTasks with mixed labels (should be empty): {mixed_tasks_fixed}\")\n",
    "\n",
    "# Verify fall task structure\n",
    "fall_tasks = merged_data_fixed[merged_data_fixed['Is_Fall'] == True]\n",
    "print(f\"\\nFall task verification:\")\n",
    "print(f\"Fall task range: {fall_tasks['Task'].min()}-{fall_tasks['Task'].max()}\")\n",
    "print(f\"Fall trials with labels: {fall_tasks['Fall_onset_frame'].notna().sum()}/{len(fall_tasks)}\")\n",
    "\n",
    "# Verify ADL task structure  \n",
    "adl_tasks = merged_data_fixed[merged_data_fixed['Is_Fall'] == False]\n",
    "print(f\"\\nADL task verification:\")\n",
    "print(f\"ADL task range: {adl_tasks['Task'].min()}-{adl_tasks['Task'].max()}\")\n",
    "print(f\"ADL tasks (should have no fall labels): {adl_tasks['Fall_onset_frame'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89ec1b72-9498-4da3-90e1-0f6efb2152f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SA06...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  SA06: 119061 samples, 41138 fall samples\n",
      "Processing SA07...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  SA07: 127137 samples, 46274 fall samples\n",
      "Processing SA08...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  SA08: 126420 samples, 48617 fall samples\n",
      "\n",
      "Combined dataset: 372618 total samples\n",
      "Subjects: ['SA06', 'SA07', 'SA08']\n",
      "Fall vs ADL: Is_Fall\n",
      "False    236589\n",
      "True     136029\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Merge multiple subjects with the fixed function\n",
    "subjects_to_merge = ['SA06', 'SA07', 'SA08']  # Start with 3\n",
    "all_subject_data = []\n",
    "\n",
    "for subject_id in subjects_to_merge:\n",
    "    print(f\"Processing {subject_id}...\")\n",
    "    subject_data = merge_sensor_with_labels_fixed(subject_id)\n",
    "    all_subject_data.append(subject_data)\n",
    "    print(f\"  {subject_id}: {len(subject_data)} samples, {subject_data['Is_Fall'].sum()} fall samples\")\n",
    "\n",
    "# Combine all subjects\n",
    "combined_dataset = pd.concat(all_subject_data, ignore_index=True)\n",
    "print(f\"\\nCombined dataset: {len(combined_dataset)} total samples\")\n",
    "print(f\"Subjects: {sorted(combined_dataset['Subject'].unique())}\")\n",
    "print(f\"Fall vs ADL: {combined_dataset['Is_Fall'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8371ad42-e14a-4f46-9e3e-3c0158882357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task coverage (sample counts per subject):\n",
      "Subject  SA06  SA07  SA08\n",
      "Task                     \n",
      "1        3072  3104  3084\n",
      "2        3627  3442  4038\n",
      "3        2834  1788  2660\n",
      "4        1703  1451  2490\n",
      "5        4406  4718  4028\n",
      "6        6641  6161  5778\n",
      "7        5212  5176  3489\n",
      "8        4042  4568  4450\n",
      "9        3366  3309  3598\n",
      "10       4195  4021  3963\n",
      "11       3202  3043  3137\n",
      "12       3025  3054  3093\n",
      "13       3678  4332  3902\n",
      "14       3040  2074  3862\n",
      "15       2183  3241  4180\n",
      "16       4070  4067  2889\n",
      "17       3147  3310  3116\n",
      "18       4464  5350  4420\n",
      "19       3817  5075  3104\n",
      "20       2573  3307  2961\n",
      "21       1847  2892  2572\n",
      "22       2565  3065  2876\n",
      "23       2737  3270  2954\n",
      "24       2732  2946  3018\n",
      "25       1822  2802  2914\n",
      "26       2548  2905  3006\n",
      "27       2072   513  2480\n",
      "28       3443  3752  3960\n",
      "29       3195  3629  4634\n",
      "30       3150   790  3562\n",
      "31       3005  2950  3156\n",
      "32       3240  4347  3564\n",
      "33       2466  4466  3651\n",
      "34       3743  4640  3309\n",
      "35       3898  5298  5382\n",
      "36       4301  4281  3140\n",
      "\n",
      "SA06: Complete task coverage (1-36)\n",
      "\n",
      "SA07: Complete task coverage (1-36)\n",
      "\n",
      "SA08: Complete task coverage (1-36)\n"
     ]
    }
   ],
   "source": [
    "# Check task coverage across your 3 subjects\n",
    "def analyze_task_coverage(combined_dataset):\n",
    "    \"\"\"Analyze which tasks each subject performed\"\"\"\n",
    "    \n",
    "    coverage_summary = combined_dataset.groupby(['Subject', 'Task']).size().reset_index(name='Samples')\n",
    "    \n",
    "    # Create a pivot table to see task coverage\n",
    "    coverage_pivot = coverage_summary.pivot(index='Task', columns='Subject', values='Samples')\n",
    "    coverage_pivot = coverage_pivot.fillna(0)  # Replace NaN with 0 for missing tasks\n",
    "    \n",
    "    print(\"Task coverage (sample counts per subject):\")\n",
    "    print(coverage_pivot)\n",
    "    \n",
    "    # Check for missing tasks\n",
    "    all_tasks = set(range(1, 37))  # Tasks 1-36\n",
    "    for subject in ['SA06', 'SA07', 'SA08']:\n",
    "        subject_tasks = set(combined_dataset[combined_dataset['Subject'] == subject]['Task'].unique())\n",
    "        missing_tasks = all_tasks - subject_tasks\n",
    "        if missing_tasks:\n",
    "            print(f\"\\n{subject} missing tasks: {sorted(missing_tasks)}\")\n",
    "        else:\n",
    "            print(f\"\\n{subject}: Complete task coverage (1-36)\")\n",
    "    \n",
    "    return coverage_pivot\n",
    "\n",
    "task_coverage = analyze_task_coverage(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe9ec8c3-c6f7-4eb8-a567-c2dcb8253f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 8-class training data following Jain & Semwal methodology...\n",
      "Processing ADL tasks...\n",
      "Processing fall tasks...\n",
      "\n",
      "Extracted 2564 training windows\n",
      "Window shape: (100, 6) (samples, features)\n",
      "Total classes found: 6\n",
      "8-Class Distribution:\n",
      "Walking: 1197\n",
      "Jogging: 840\n",
      "Stairs: 0\n",
      "Stumble: 433\n",
      "Fall_Recovery: 0\n",
      "Fall_Initiation: 4\n",
      "Impact: 45\n",
      "Aftermath: 45\n",
      "\n",
      "Creating transitional windows for early fall detection...\n",
      "Transitional windows: 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class JainSemwalFeatureExtractor:\n",
    "    def __init__(self, sampling_rate=100):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.Ws = 25  # Window size adapted for 100Hz (0.25s)\n",
    "        \n",
    "        # Exact 8-class taxonomy from Jain & Semwal paper\n",
    "        self.target_classes = [\n",
    "            'Walking',           # Class 1: Combined normal + quick walking\n",
    "            'Jogging',           # Class 2: Combined normal + quick jogging  \n",
    "            'Stairs',            # Class 3: Walking upstairs/downstairs\n",
    "            'Stumble',           # Class 4: Stumble while walking\n",
    "            'Fall_Recovery',     # Class 5: Extracted from fall data\n",
    "            'Fall_Initiation',   # Class 6: Extracted from fall data  \n",
    "            'Impact',            # Class 7: Extracted from fall data\n",
    "            'Aftermath'          # Class 8: Extracted from fall data\n",
    "        ]\n",
    "        \n",
    "        # Map your task numbers to ADL classes (need verification based on your tasks)\n",
    "        self.adl_mapping = {\n",
    "            'Walking': [6, 7],     # T06: Walk normally, T07: Walk quickly\n",
    "            'Jogging': [8, 9],     # T08: Jog normally, T09: Jog quickly  \n",
    "            'Stairs': [20, 21],    # Need to check - these might be fall tasks\n",
    "            'Stumble': [10]        # T10: Stumble while walking\n",
    "        }\n",
    "        \n",
    "    def extract_fall_phases_8class(self, trial_data):\n",
    "        \"\"\"\n",
    "        Extract the 4 fall phases exactly as described in Jain & Semwal paper\n",
    "        \"\"\"\n",
    "        if not trial_data['Is_Fall'].iloc[0]:\n",
    "            return {}\n",
    "            \n",
    "        onset_frame = trial_data['Fall_onset_frame'].iloc[0]\n",
    "        impact_frame = trial_data['Fall_impact_frame'].iloc[0]\n",
    "        \n",
    "        if pd.isna(onset_frame) or pd.isna(impact_frame):\n",
    "            return {}\n",
    "            \n",
    "        onset_frame = int(onset_frame)\n",
    "        impact_frame = int(impact_frame)\n",
    "        \n",
    "        sensor_cols = ['AccX', 'AccY', 'AccZ', 'GyrX', 'GyrY', 'GyrZ']\n",
    "        phases = {}\n",
    "        \n",
    "        # Following their Algorithm 1 approach but using ground truth labels\n",
    "        \n",
    "        # Fall Recovery: Activity before fall onset (if sufficient data)\n",
    "        if onset_frame > 50:  # Need some data before onset\n",
    "            # Extract last part of pre-fall activity as \"recovery-like\" movement\n",
    "            recovery_start = max(0, onset_frame - 2*self.Ws)  # 0.5s before onset\n",
    "            phases['Fall_Recovery'] = trial_data.iloc[recovery_start:onset_frame][sensor_cols].values\n",
    "            \n",
    "        # Fall Initiation: From onset to impact\n",
    "        fall_duration = impact_frame - onset_frame\n",
    "        if fall_duration > 0:\n",
    "            phases['Fall_Initiation'] = trial_data.iloc[onset_frame:impact_frame][sensor_cols].values\n",
    "            \n",
    "        # Impact: 1 second after impact starts  \n",
    "        impact_end = min(impact_frame + self.Ws*4, len(trial_data))  # 4*Ws = 1 second at 100Hz\n",
    "        if impact_frame < len(trial_data):\n",
    "            phases['Impact'] = trial_data.iloc[impact_frame:impact_end][sensor_cols].values\n",
    "            \n",
    "        # Aftermath: After impact phase\n",
    "        if impact_end < len(trial_data):\n",
    "            aftermath_end = min(impact_end + self.Ws*4, len(trial_data))  # Another 1 second\n",
    "            if aftermath_end > impact_end:\n",
    "                phases['Aftermath'] = trial_data.iloc[impact_end:aftermath_end][sensor_cols].values\n",
    "                \n",
    "        return phases\n",
    "    \n",
    "    def extract_adl_20s_segments(self, trial_data, target_duration=20):\n",
    "        \"\"\"\n",
    "        Extract 20-second segments from ADL activities as per their methodology\n",
    "        \"\"\"\n",
    "        sensor_cols = ['AccX', 'AccY', 'AccZ', 'GyrX', 'GyrY', 'GyrZ']\n",
    "        data = trial_data[sensor_cols].values\n",
    "        \n",
    "        target_samples = target_duration * self.sampling_rate  # 2000 samples for 20s\n",
    "        \n",
    "        if len(data) < target_samples:\n",
    "            # If shorter than 20s, pad or use available data\n",
    "            return data if len(data) >= self.sampling_rate else None  # At least 1 second\n",
    "        else:\n",
    "            # Extract 20-second segment from middle\n",
    "            start_idx = (len(data) - target_samples) // 2\n",
    "            end_idx = start_idx + target_samples\n",
    "            return data[start_idx:end_idx]\n",
    "    \n",
    "    def create_8class_training_data(self, combined_dataset, window_size=100):\n",
    "        \"\"\"\n",
    "        Create training data for exact 8-class taxonomy from Jain & Semwal\n",
    "        Returns (features, labels) where features are (N, 100, 6) and labels are 8 classes\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        labels = []\n",
    "        metadata = []\n",
    "        \n",
    "        print(\"Processing ADL tasks...\")\n",
    "        # Process ADL tasks (T01-T19, T35-T36)\n",
    "        adl_data = combined_dataset[combined_dataset['Is_Fall'] == False]\n",
    "        \n",
    "        for (subject, task, trial), trial_data in adl_data.groupby(['Subject', 'Task', 'Trial']):\n",
    "            \n",
    "            # Map task to ADL class\n",
    "            adl_class = None\n",
    "            for class_name, task_list in self.adl_mapping.items():\n",
    "                if task in task_list:\n",
    "                    adl_class = class_name\n",
    "                    break\n",
    "                    \n",
    "            if adl_class:\n",
    "                # Extract 20-second segment\n",
    "                segment = self.extract_adl_20s_segments(trial_data)\n",
    "                if segment is not None and len(segment) >= window_size:\n",
    "                    # Create overlapping windows\n",
    "                    windows = self.create_overlapping_windows(segment, window_size)\n",
    "                    for window in windows:\n",
    "                        features.append(window)\n",
    "                        labels.append(adl_class)\n",
    "                        metadata.append({\n",
    "                            'Subject': subject, 'Task': task, 'Trial': trial, \n",
    "                            'Class': adl_class, 'Type': 'ADL'\n",
    "                        })\n",
    "        \n",
    "        print(\"Processing fall tasks...\")\n",
    "        # Process fall tasks (T20-T34)  \n",
    "        fall_data = combined_dataset[combined_dataset['Is_Fall'] == True]\n",
    "        \n",
    "        for (subject, task, trial), trial_data in fall_data.groupby(['Subject', 'Task', 'Trial']):\n",
    "            \n",
    "            # Extract all 4 fall phases\n",
    "            phases = self.extract_fall_phases_8class(trial_data)\n",
    "            \n",
    "            for phase_name, phase_data in phases.items():\n",
    "                if len(phase_data) >= window_size:\n",
    "                    windows = self.create_overlapping_windows(phase_data, window_size)\n",
    "                    for window in windows:\n",
    "                        features.append(window)\n",
    "                        labels.append(phase_name)\n",
    "                        metadata.append({\n",
    "                            'Subject': subject, 'Task': task, 'Trial': trial,\n",
    "                            'Class': phase_name, 'Type': 'Fall'\n",
    "                        })\n",
    "        \n",
    "        return np.array(features), labels, metadata\n",
    "    \n",
    "    def create_overlapping_windows(self, data, window_size, overlap=0.75):\n",
    "        \"\"\"\n",
    "        Create overlapping windows - higher overlap for more training data\n",
    "        \"\"\"\n",
    "        windows = []\n",
    "        step_size = int(window_size * (1 - overlap))\n",
    "        \n",
    "        for i in range(0, len(data) - window_size + 1, step_size):\n",
    "            window = data[i:i + window_size]\n",
    "            if window.shape[0] == window_size:  # Ensure exact window size\n",
    "                windows.append(window)\n",
    "                \n",
    "        return windows\n",
    "    \n",
    "    def create_transitional_windows(self, combined_dataset, window_size=50):\n",
    "        \"\"\"\n",
    "        Create transitional windows (Tw) for early fall detection as per their paper\n",
    "        This is their key innovation for 0.5s reaction time\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        fall_data = combined_dataset[combined_dataset['Is_Fall'] == True]\n",
    "        \n",
    "        for (subject, task, trial), trial_data in fall_data.groupby(['Subject', 'Task', 'Trial']):\n",
    "            \n",
    "            onset_frame = trial_data['Fall_onset_frame'].iloc[0]\n",
    "            impact_frame = trial_data['Fall_impact_frame'].iloc[0]\n",
    "            \n",
    "            if not (pd.isna(onset_frame) or pd.isna(impact_frame)):\n",
    "                onset_frame = int(onset_frame)\n",
    "                impact_frame = int(impact_frame)\n",
    "                \n",
    "                # Transitional window: first half of fall initiation phase\n",
    "                fall_duration = impact_frame - onset_frame\n",
    "                tw_end = onset_frame + fall_duration // 2\n",
    "                \n",
    "                if tw_end > onset_frame and (tw_end - onset_frame) >= window_size:\n",
    "                    sensor_cols = ['AccX', 'AccY', 'AccZ', 'GyrX', 'GyrY', 'GyrZ']\n",
    "                    tw_data = trial_data.iloc[onset_frame:tw_end][sensor_cols].values\n",
    "                    \n",
    "                    windows = self.create_overlapping_windows(tw_data, window_size)\n",
    "                    for window in windows:\n",
    "                        features.append(window)\n",
    "                        labels.append('Fall_Initiation')  # Label as fall initiation\n",
    "        \n",
    "        return np.array(features), labels\n",
    "    \n",
    "    def analyze_class_distribution(self, labels):\n",
    "        \"\"\"Analyze the distribution of the 8 classes\"\"\"\n",
    "        class_counts = pd.Series(labels).value_counts()\n",
    "        \n",
    "        print(\"8-Class Distribution:\")\n",
    "        for class_name in self.target_classes:\n",
    "            count = class_counts.get(class_name, 0)\n",
    "            print(f\"{class_name}: {count}\")\n",
    "            \n",
    "        return class_counts\n",
    "\n",
    "# Usage\n",
    "extractor = JainSemwalFeatureExtractor()\n",
    "\n",
    "print(\"Creating 8-class training data following Jain & Semwal methodology...\")\n",
    "features, labels, metadata = extractor.create_8class_training_data(combined_dataset)\n",
    "\n",
    "print(f\"\\nExtracted {len(features)} training windows\")\n",
    "print(f\"Window shape: {features[0].shape} (samples, features)\")\n",
    "print(f\"Total classes found: {len(set(labels))}\")\n",
    "\n",
    "# Analyze class distribution\n",
    "extractor.analyze_class_distribution(labels)\n",
    "\n",
    "# Create transitional windows for early detection\n",
    "print(\"\\nCreating transitional windows for early fall detection...\")\n",
    "tw_features, tw_labels = extractor.create_transitional_windows(combined_dataset)\n",
    "print(f\"Transitional windows: {len(tw_features)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73d197a8-7356-496a-bf6a-a56ce59b82cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total fall trials: 210\n",
      "Labeled fall trials: 45\n",
      "SSA06 T20 R1: Fall duration = 78 frames (0.78s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA06 T21 R1: Fall duration = 39 frames (0.39s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA06 T22 R1: Fall duration = 66 frames (0.66s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA06 T23 R1: Fall duration = 86 frames (0.86s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA06 T24 R1: Fall duration = 71 frames (0.71s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA06 T25 R2: Fall duration = 104 frames (1.04s)\n",
      "SSA06 T26 R1: Fall duration = 76 frames (0.76s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA06 T27 R1: Fall duration = 38 frames (0.38s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA06 T28 R1: Fall duration = 57 frames (0.57s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA06 T29 R1: Fall duration = 86 frames (0.86s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA06 T30 R1: Fall duration = 75 frames (0.75s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA06 T31 R1: Fall duration = 75 frames (0.75s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA06 T32 R1: Fall duration = 49 frames (0.49s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA06 T33 R1: Fall duration = 59 frames (0.59s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA06 T34 R1: Fall duration = 65 frames (0.65s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA07 T20 R1: Fall duration = 100 frames (1.00s)\n",
      "SSA07 T21 R1: Fall duration = 50 frames (0.50s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA07 T22 R1: Fall duration = 83 frames (0.83s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA07 T23 R1: Fall duration = 87 frames (0.87s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA07 T24 R1: Fall duration = 93 frames (0.93s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA07 T25 R1: Fall duration = 97 frames (0.97s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA07 T26 R1: Fall duration = 88 frames (0.88s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA07 T27 R4: Fall duration = 48 frames (0.48s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA07 T28 R1: Fall duration = 86 frames (0.86s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA07 T29 R2: Fall duration = 69 frames (0.69s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA07 T30 R6: Fall duration = 81 frames (0.81s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA07 T31 R2: Fall duration = 79 frames (0.79s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA07 T32 R1: Fall duration = 76 frames (0.76s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA07 T33 R1: Fall duration = 79 frames (0.79s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA07 T34 R1: Fall duration = 68 frames (0.68s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA08 T20 R2: Fall duration = 103 frames (1.03s)\n",
      "SSA08 T21 R1: Fall duration = 44 frames (0.44s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA08 T22 R1: Fall duration = 56 frames (0.56s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA08 T23 R1: Fall duration = 72 frames (0.72s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA08 T24 R1: Fall duration = 47 frames (0.47s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA08 T25 R1: Fall duration = 115 frames (1.15s)\n",
      "SSA08 T26 R1: Fall duration = 86 frames (0.86s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA08 T27 R1: Fall duration = 46 frames (0.46s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA08 T28 R1: Fall duration = 83 frames (0.83s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA08 T29 R1: Fall duration = 64 frames (0.64s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA08 T30 R1: Fall duration = 68 frames (0.68s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA08 T31 R1: Fall duration = 62 frames (0.62s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA08 T32 R1: Fall duration = 54 frames (0.54s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA08 T33 R1: Fall duration = 56 frames (0.56s)\n",
      "  ⚠️  Too short for 100-sample windows\n",
      "SSA08 T34 R1: Fall duration = 63 frames (0.63s)\n",
      "  ⚠️  Too short for 100-sample windows\n"
     ]
    }
   ],
   "source": [
    "# Debug the fall phase extraction\n",
    "fall_data = combined_dataset[combined_dataset['Is_Fall'] == True]\n",
    "labeled_falls = fall_data[fall_data['Fall_onset_frame'].notna()]\n",
    "\n",
    "print(f\"Total fall trials: {len(fall_data.groupby(['Subject', 'Task', 'Trial']))}\")\n",
    "print(f\"Labeled fall trials: {len(labeled_falls.groupby(['Subject', 'Task', 'Trial']))}\")\n",
    "\n",
    "# Check fall phase durations\n",
    "for (subject, task, trial), trial_data in labeled_falls.groupby(['Subject', 'Task', 'Trial']):\n",
    "    onset = int(trial_data['Fall_onset_frame'].iloc[0])\n",
    "    impact = int(trial_data['Fall_impact_frame'].iloc[0])\n",
    "    fall_duration = impact - onset\n",
    "    print(f\"S{subject} T{task} R{trial}: Fall duration = {fall_duration} frames ({fall_duration/100:.2f}s)\")\n",
    "    if fall_duration < 100:\n",
    "        print(f\"  ⚠️  Too short for 100-sample windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd790952-3fbd-45c4-9778-117fb8e3ce53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ADL tasks...\n",
      "Processing fall tasks...\n"
     ]
    }
   ],
   "source": [
    "# Use 50-sample windows (0.5 seconds) instead of 100\n",
    "extractor = JainSemwalFeatureExtractor()\n",
    "features, labels, metadata = extractor.create_8class_training_data(\n",
    "    combined_dataset, \n",
    "    window_size=50  # Changed from 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79980ef6-3b7c-4692-9f99-8fe72dbda3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with SA06 only: 119061 samples\n",
      "Processing ADL tasks...\n",
      "Processing fall tasks...\n",
      "SA06 results: 2068 windows\n",
      "Classes: {'Aftermath', 'Impact', 'Fall_Recovery', 'Fall_Initiation', 'Stumble', 'Jogging', 'Walking'}\n"
     ]
    }
   ],
   "source": [
    "# Add debugging to see where it hangs\n",
    "extractor = JainSemwalFeatureExtractor()\n",
    "\n",
    "# Test with just one subject first\n",
    "sa06_data = combined_dataset[combined_dataset['Subject'] == 'SA06']\n",
    "print(f\"Testing with SA06 only: {len(sa06_data)} samples\")\n",
    "\n",
    "features, labels, metadata = extractor.create_8class_training_data(\n",
    "    sa06_data, \n",
    "    window_sijjze=50\n",
    ")\n",
    "\n",
    "print(f\"SA06 results: {len(features)} windows\")\n",
    "print(f\"Classes: {set(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbf17efc-8c9b-41b8-810e-0bebbfc75d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-Class Distribution:\n",
      "Walking: 952\n",
      "Jogging: 588\n",
      "Stairs: 0\n",
      "Stumble: 331\n",
      "Fall_Recovery: 14\n",
      "Fall_Initiation: 33\n",
      "Impact: 75\n",
      "Aftermath: 75\n",
      "\n",
      "Processing SA07...\n",
      "Processing ADL tasks...\n",
      "Processing fall tasks...\n",
      "\n",
      "Processing SA08...\n",
      "Processing ADL tasks...\n",
      "Processing fall tasks...\n",
      "\n",
      "Combined dataset: 6009 windows\n",
      "Final classes: {'Aftermath', 'Impact', 'Fall_Recovery', 'Fall_Initiation', 'Stumble', 'Jogging', 'Walking'}\n"
     ]
    }
   ],
   "source": [
    "# Analyze SA06 class distribution\n",
    "extractor.analyze_class_distribution(labels)\n",
    "\n",
    "# Then process other subjects individually and combine\n",
    "print(\"\\nProcessing SA07...\")\n",
    "sa07_data = combined_dataset[combined_dataset['Subject'] == 'SA07']\n",
    "features_07, labels_07, metadata_07 = extractor.create_8class_training_data(sa07_data, window_size=50)\n",
    "\n",
    "print(\"\\nProcessing SA08...\")\n",
    "sa08_data = combined_dataset[combined_dataset['Subject'] == 'SA08']\n",
    "features_08, labels_08, metadata_08 = extractor.create_8class_training_data(sa08_data, window_size=50)\n",
    "\n",
    "# Combine all subjects\n",
    "all_features = np.concatenate([features, features_07, features_08])\n",
    "all_labels = labels + labels_07 + labels_08\n",
    "all_metadata = metadata + metadata_07 + metadata_08\n",
    "\n",
    "print(f\"\\nCombined dataset: {len(all_features)} windows\")\n",
    "print(f\"Final classes: {set(all_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34e06bf1-7d63-4218-9f8c-9848fc23e598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 21:41:22.538815: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-16 21:41:22.774678: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-16 21:41:26.246244: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Aftermath' 'Fall_Initiation' 'Fall_Recovery' 'Impact' 'Jogging'\n",
      " 'Stumble' 'Walking']\n",
      "Number of classes: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758080487.221099    3717 cuda_executor.cc:1309] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1758080487.228921    3717 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">231</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m2,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m394,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m231\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">440,135</span> (1.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m440,135\u001b[0m (1.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">440,135</span> (1.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m440,135\u001b[0m (1.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare labels for training\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(all_labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"Classes: {label_encoder.classes_}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Build CNN-LSTM architecture (adapted for 50Hz input)\n",
    "model = Sequential([\n",
    "    Conv1D(128, 3, activation='relu', input_shape=(50, 6)),\n",
    "    MaxPooling1D(2),\n",
    "    LSTM(256),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "922f52c5-0313-4e28-9cf8-58a8d4ea97ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4807\n",
      "Test samples: 1202\n",
      "Epoch 1/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.5862 - loss: 1.0492 - val_accuracy: 0.6339 - val_loss: 0.9031\n",
      "Epoch 2/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.6892 - loss: 0.7732 - val_accuracy: 0.7047 - val_loss: 0.7497\n",
      "Epoch 3/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.7185 - loss: 0.6798 - val_accuracy: 0.7105 - val_loss: 0.6711\n",
      "Epoch 4/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.7412 - loss: 0.6299 - val_accuracy: 0.7354 - val_loss: 0.6305\n",
      "Epoch 5/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.7447 - loss: 0.6065 - val_accuracy: 0.7255 - val_loss: 0.6415\n",
      "Epoch 6/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.7678 - loss: 0.5528 - val_accuracy: 0.7155 - val_loss: 0.6384\n",
      "Epoch 7/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.7814 - loss: 0.5319 - val_accuracy: 0.7521 - val_loss: 0.5967\n",
      "Epoch 8/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.7986 - loss: 0.5029 - val_accuracy: 0.7812 - val_loss: 0.5326\n",
      "Epoch 9/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.8169 - loss: 0.4517 - val_accuracy: 0.7795 - val_loss: 0.5245\n",
      "Epoch 10/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8088 - loss: 0.4601 - val_accuracy: 0.7679 - val_loss: 0.5444\n",
      "Epoch 11/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.8296 - loss: 0.4131 - val_accuracy: 0.7995 - val_loss: 0.4881\n",
      "Epoch 12/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.8461 - loss: 0.3792 - val_accuracy: 0.8020 - val_loss: 0.4786\n",
      "Epoch 13/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.8546 - loss: 0.3581 - val_accuracy: 0.7987 - val_loss: 0.5008\n",
      "Epoch 14/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8488 - loss: 0.3731 - val_accuracy: 0.8020 - val_loss: 0.4793\n",
      "Epoch 15/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.8687 - loss: 0.3207 - val_accuracy: 0.8020 - val_loss: 0.5535\n",
      "Epoch 16/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8664 - loss: 0.3303 - val_accuracy: 0.7978 - val_loss: 0.5704\n",
      "Epoch 17/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8752 - loss: 0.2935 - val_accuracy: 0.8145 - val_loss: 0.4886\n",
      "Epoch 18/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.8997 - loss: 0.2529 - val_accuracy: 0.8153 - val_loss: 0.5183\n",
      "Epoch 19/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8949 - loss: 0.2664 - val_accuracy: 0.8236 - val_loss: 0.5035\n",
      "Epoch 20/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9053 - loss: 0.2462 - val_accuracy: 0.8186 - val_loss: 0.4669\n",
      "Epoch 21/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9162 - loss: 0.2176 - val_accuracy: 0.8286 - val_loss: 0.4598\n",
      "Epoch 22/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9241 - loss: 0.1827 - val_accuracy: 0.8444 - val_loss: 0.4498\n",
      "Epoch 23/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9280 - loss: 0.1804 - val_accuracy: 0.8161 - val_loss: 0.5692\n",
      "Epoch 24/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9189 - loss: 0.2020 - val_accuracy: 0.8286 - val_loss: 0.4681\n",
      "Epoch 25/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9309 - loss: 0.1710 - val_accuracy: 0.8178 - val_loss: 0.5140\n",
      "Epoch 26/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9255 - loss: 0.1807 - val_accuracy: 0.8386 - val_loss: 0.4737\n",
      "Epoch 27/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9324 - loss: 0.1643 - val_accuracy: 0.8344 - val_loss: 0.4472\n",
      "Epoch 28/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9474 - loss: 0.1334 - val_accuracy: 0.8369 - val_loss: 0.5216\n",
      "Epoch 29/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9407 - loss: 0.1618 - val_accuracy: 0.8253 - val_loss: 0.5902\n",
      "Epoch 30/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9538 - loss: 0.1133 - val_accuracy: 0.8353 - val_loss: 0.5114\n",
      "Epoch 31/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.9584 - loss: 0.1093 - val_accuracy: 0.8344 - val_loss: 0.5909\n",
      "Epoch 32/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9603 - loss: 0.1009 - val_accuracy: 0.8403 - val_loss: 0.5779\n",
      "Epoch 33/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9484 - loss: 0.1391 - val_accuracy: 0.8328 - val_loss: 0.5315\n",
      "Epoch 34/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.9515 - loss: 0.1278 - val_accuracy: 0.8170 - val_loss: 0.6243\n",
      "Epoch 35/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9505 - loss: 0.1233 - val_accuracy: 0.8486 - val_loss: 0.4687\n",
      "Epoch 36/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.9586 - loss: 0.1079 - val_accuracy: 0.8552 - val_loss: 0.4934\n",
      "Epoch 37/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9615 - loss: 0.1082 - val_accuracy: 0.8444 - val_loss: 0.4820\n",
      "Epoch 38/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9678 - loss: 0.0763 - val_accuracy: 0.8486 - val_loss: 0.5447\n",
      "Epoch 39/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9636 - loss: 0.1048 - val_accuracy: 0.8469 - val_loss: 0.5261\n",
      "Epoch 40/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9717 - loss: 0.0714 - val_accuracy: 0.8511 - val_loss: 0.5157\n",
      "Epoch 41/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9667 - loss: 0.0919 - val_accuracy: 0.8494 - val_loss: 0.5351\n",
      "Epoch 42/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9738 - loss: 0.0690 - val_accuracy: 0.8411 - val_loss: 0.6678\n",
      "Epoch 43/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9648 - loss: 0.1004 - val_accuracy: 0.8328 - val_loss: 0.5821\n",
      "Epoch 44/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.9651 - loss: 0.1017 - val_accuracy: 0.8461 - val_loss: 0.5809\n",
      "Epoch 45/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.9709 - loss: 0.0789 - val_accuracy: 0.8486 - val_loss: 0.5723\n",
      "Epoch 46/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.9786 - loss: 0.0563 - val_accuracy: 0.8428 - val_loss: 0.6033\n",
      "Epoch 47/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9836 - loss: 0.0449 - val_accuracy: 0.8444 - val_loss: 0.6127\n",
      "Epoch 48/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9759 - loss: 0.0721 - val_accuracy: 0.8586 - val_loss: 0.5277\n",
      "Epoch 49/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9692 - loss: 0.0845 - val_accuracy: 0.8428 - val_loss: 0.5606\n",
      "Epoch 50/50\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.9834 - loss: 0.0497 - val_accuracy: 0.8461 - val_loss: 0.6584\n",
      "\n",
      "Test Accuracy: 0.8461\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Aftermath       0.96      0.98      0.97        45\n",
      "Fall_Initiation       1.00      0.86      0.93        22\n",
      "  Fall_Recovery       0.44      0.44      0.44         9\n",
      "         Impact       0.93      0.93      0.93        45\n",
      "        Jogging       0.86      0.88      0.87       369\n",
      "        Stumble       0.81      0.58      0.67       192\n",
      "        Walking       0.83      0.91      0.87       520\n",
      "\n",
      "       accuracy                           0.85      1202\n",
      "      macro avg       0.83      0.80      0.81      1202\n",
      "   weighted avg       0.85      0.85      0.84      1202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data for training and validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_features, encoded_labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=encoded_labels\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate performance\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Get detailed classification report\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53b4e652-8337-4133-be63-1433196232b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 200Hz upsampled training data (exact Jain & Semwal replication)...\n",
      "Processing ADL tasks with 200Hz upsampling...\n",
      "Processing fall tasks with 200Hz upsampling...\n",
      "\n",
      "Extracted 2564 training windows\n",
      "Window shape: (200, 6) (should be 200, 6)\n",
      "Total classes found: 6\n",
      "\n",
      "Class Distribution (200Hz):\n",
      "Walking: 1197\n",
      "Jogging: 840\n",
      "Stairs: 0\n",
      "Stumble: 433\n",
      "Fall_Recovery: 0\n",
      "Fall_Initiation: 4\n",
      "Impact: 45\n",
      "Aftermath: 45\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class JainSemwalReplicator:\n",
    "    def __init__(self, target_sampling_rate=200):\n",
    "        self.original_rate = 100\n",
    "        self.target_rate = target_sampling_rate\n",
    "        self.Ws = 50  # Window size for 200Hz (0.25s)\n",
    "        \n",
    "        # Exact replication of their 8-class taxonomy\n",
    "        self.target_classes = [\n",
    "            'Walking',           # Class 1\n",
    "            'Jogging',           # Class 2  \n",
    "            'Stairs',            # Class 3\n",
    "            'Stumble',           # Class 4\n",
    "            'Fall_Recovery',     # Class 5\n",
    "            'Fall_Initiation',   # Class 6\n",
    "            'Impact',            # Class 7\n",
    "            'Aftermath'          # Class 8\n",
    "        ]\n",
    "        \n",
    "        # ADL mapping - need to verify against your task analysis\n",
    "        self.adl_mapping = {\n",
    "            'Walking': [6, 7],     # T06: Walk normally, T07: Walk quickly\n",
    "            'Jogging': [8, 9],     # T08: Jog normally, T09: Jog quickly  \n",
    "            'Stumble': [10],       # T10: Stumble while walking\n",
    "            # Stairs: Need to identify from your T01-T19 range\n",
    "        }\n",
    "        \n",
    "    def upsample_to_200hz(self, data):\n",
    "        \"\"\"\n",
    "        Upsample 100Hz data to 200Hz using cubic spline interpolation\n",
    "        Exactly as described in Jain & Semwal paper\n",
    "        \"\"\"\n",
    "        if len(data) < 2:\n",
    "            return data\n",
    "            \n",
    "        # Create time vectors\n",
    "        original_time = np.linspace(0, 1, len(data))\n",
    "        upsampled_time = np.linspace(0, 1, len(data) * 2)\n",
    "        \n",
    "        # Interpolate each sensor channel separately\n",
    "        upsampled_data = np.zeros((len(upsampled_time), data.shape[1]))\n",
    "        \n",
    "        for i in range(data.shape[1]):\n",
    "            interpolator = interpolate.interp1d(\n",
    "                original_time, data[:, i], \n",
    "                kind='cubic', \n",
    "                bounds_error=False, \n",
    "                fill_value='extrapolate'\n",
    "            )\n",
    "            upsampled_data[:, i] = interpolator(upsampled_time)\n",
    "            \n",
    "        return upsampled_data\n",
    "    \n",
    "    def extract_fall_phases_200hz(self, trial_data):\n",
    "        \"\"\"\n",
    "        Extract fall phases using their exact methodology with 200Hz data\n",
    "        \"\"\"\n",
    "        if not trial_data['Is_Fall'].iloc[0]:\n",
    "            return {}\n",
    "            \n",
    "        onset_frame = trial_data['Fall_onset_frame'].iloc[0]\n",
    "        impact_frame = trial_data['Fall_impact_frame'].iloc[0]\n",
    "        \n",
    "        if pd.isna(onset_frame) or pd.isna(impact_frame):\n",
    "            return {}\n",
    "            \n",
    "        # Upsample sensor data first\n",
    "        sensor_cols = ['AccX', 'AccY', 'AccZ', 'GyrX', 'GyrY', 'GyrZ']\n",
    "        original_data = trial_data[sensor_cols].values\n",
    "        upsampled_data = self.upsample_to_200hz(original_data)\n",
    "        \n",
    "        # Scale frame indices for 200Hz\n",
    "        onset_frame_200hz = int(onset_frame * 2)\n",
    "        impact_frame_200hz = int(impact_frame * 2)\n",
    "        \n",
    "        phases = {}\n",
    "        \n",
    "        # Following their Algorithm 1 exactly with 200Hz timing\n",
    "        \n",
    "        # Fall Recovery: Activity before fall onset\n",
    "        if onset_frame_200hz > 100:  # Need sufficient data at 200Hz\n",
    "            recovery_start = max(0, onset_frame_200hz - 2*self.Ws)  # 0.5s before onset\n",
    "            phases['Fall_Recovery'] = upsampled_data[recovery_start:onset_frame_200hz]\n",
    "            \n",
    "        # Fall Initiation: From onset to impact  \n",
    "        fall_duration = impact_frame_200hz - onset_frame_200hz\n",
    "        if fall_duration > 0:\n",
    "            phases['Fall_Initiation'] = upsampled_data[onset_frame_200hz:impact_frame_200hz]\n",
    "            \n",
    "        # Impact: 1 second after impact (4*Ws = 200 samples at 200Hz)\n",
    "        impact_end = min(impact_frame_200hz + 4*self.Ws, len(upsampled_data))\n",
    "        if impact_frame_200hz < len(upsampled_data):\n",
    "            phases['Impact'] = upsampled_data[impact_frame_200hz:impact_end]\n",
    "            \n",
    "        # Aftermath: After impact phase\n",
    "        if impact_end < len(upsampled_data):\n",
    "            aftermath_end = min(impact_end + 4*self.Ws, len(upsampled_data))\n",
    "            if aftermath_end > impact_end:\n",
    "                phases['Aftermath'] = upsampled_data[impact_end:aftermath_end]\n",
    "                \n",
    "        return phases\n",
    "    \n",
    "    def extract_adl_20s_segments_200hz(self, trial_data, target_duration=20):\n",
    "        \"\"\"\n",
    "        Extract 20-second segments and upsample to 200Hz\n",
    "        \"\"\"\n",
    "        sensor_cols = ['AccX', 'AccY', 'AccZ', 'GyrX', 'GyrY', 'GyrZ']\n",
    "        original_data = trial_data[sensor_cols].values\n",
    "        \n",
    "        # Extract 20s at original 100Hz (2000 samples)\n",
    "        target_samples_100hz = target_duration * self.original_rate\n",
    "        \n",
    "        if len(original_data) < target_samples_100hz:\n",
    "            if len(original_data) < self.original_rate:  # Less than 1 second\n",
    "                return None\n",
    "            segment_100hz = original_data\n",
    "        else:\n",
    "            # Extract middle 20 seconds\n",
    "            start_idx = (len(original_data) - target_samples_100hz) // 2\n",
    "            end_idx = start_idx + target_samples_100hz\n",
    "            segment_100hz = original_data[start_idx:end_idx]\n",
    "        \n",
    "        # Upsample to 200Hz\n",
    "        upsampled_segment = self.upsample_to_200hz(segment_100hz)\n",
    "        return upsampled_segment\n",
    "    \n",
    "    def create_200hz_training_data(self, combined_dataset, window_size=200):\n",
    "        \"\"\"\n",
    "        Create training data with 200Hz upsampling exactly matching their approach\n",
    "        Input shape will be (N, 200, 6) for 1-second windows\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        labels = []\n",
    "        metadata = []\n",
    "        \n",
    "        print(\"Processing ADL tasks with 200Hz upsampling...\")\n",
    "        adl_data = combined_dataset[combined_dataset['Is_Fall'] == False]\n",
    "        \n",
    "        for (subject, task, trial), trial_data in adl_data.groupby(['Subject', 'Task', 'Trial']):\n",
    "            \n",
    "            # Map task to ADL class\n",
    "            adl_class = None\n",
    "            for class_name, task_list in self.adl_mapping.items():\n",
    "                if task in task_list:\n",
    "                    adl_class = class_name\n",
    "                    break\n",
    "                    \n",
    "            if adl_class:\n",
    "                # Extract and upsample 20-second segment\n",
    "                segment_200hz = self.extract_adl_20s_segments_200hz(trial_data)\n",
    "                if segment_200hz is not None and len(segment_200hz) >= window_size:\n",
    "                    # Create overlapping windows\n",
    "                    windows = self.create_overlapping_windows(segment_200hz, window_size)\n",
    "                    for window in windows:\n",
    "                        features.append(window)\n",
    "                        labels.append(adl_class)\n",
    "                        metadata.append({\n",
    "                            'Subject': subject, 'Task': task, 'Trial': trial,\n",
    "                            'Class': adl_class, 'Type': 'ADL'\n",
    "                        })\n",
    "        \n",
    "        print(\"Processing fall tasks with 200Hz upsampling...\")\n",
    "        fall_data = combined_dataset[combined_dataset['Is_Fall'] == True]\n",
    "        \n",
    "        for (subject, task, trial), trial_data in fall_data.groupby(['Subject', 'Task', 'Trial']):\n",
    "            \n",
    "            # Extract all 4 fall phases with 200Hz upsampling\n",
    "            phases = self.extract_fall_phases_200hz(trial_data)\n",
    "            \n",
    "            for phase_name, phase_data in phases.items():\n",
    "                if len(phase_data) >= window_size:\n",
    "                    windows = self.create_overlapping_windows(phase_data, window_size)\n",
    "                    for window in windows:\n",
    "                        features.append(window)\n",
    "                        labels.append(phase_name)\n",
    "                        metadata.append({\n",
    "                            'Subject': subject, 'Task': task, 'Trial': trial,\n",
    "                            'Class': phase_name, 'Type': 'Fall'\n",
    "                        })\n",
    "        \n",
    "        return np.array(features), labels, metadata\n",
    "    \n",
    "    def create_overlapping_windows(self, data, window_size, overlap=0.75):\n",
    "        \"\"\"Create overlapping windows - same as before but for 200Hz data\"\"\"jj\n",
    "        windows = []\n",
    "        step_size = int(window_size * (1 - overlap))\n",
    "        \n",
    "        for i in range(0, len(data) - window_size + 1, step_size):\n",
    "            window = data[i:i + window_size]\n",
    "            if window.shape[0] == window_size:\n",
    "                windows.append(window)\n",
    "                \n",
    "        return windows\n",
    "    jk\n",
    "    def create_transitional_windows_200hz(self, combined_dataset, window_size=100):\n",
    "        \"\"\"\n",
    "        Create transitional windows with 200Hz upsampling\n",
    "        Their key innovation for 0.5s reaction time\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        fall_data = combined_dataset[combined_dataset['Is_Fall'] == True]\n",
    "        \n",
    "        for (subject, task, trial), trial_data in fall_data.groupby(['Subject', 'Task', 'Trial']):\n",
    "            \n",
    "            onset_frame = trial_data['Fall_onset_frame'].iloc[0]\n",
    "            impact_frame = trial_data['Fall_impact_frame'].iloc[0]\n",
    "            \n",
    "            if not (pd.isna(onset_frame) or pd.isna(impact_frame)):\n",
    "                # Upsample data first\n",
    "                sensor_cols = ['AccX', 'AccY', 'AccZ', 'GyrX', 'GyrY', 'GyrZ']\n",
    "                original_data = trial_data[sensor_cols].values\n",
    "                upsampled_data = self.upsample_to_200hz(original_data)\n",
    "                \n",
    "                # Scale frame indices for 200Hz\n",
    "                onset_frame_200hz = int(onset_frame * 2)\n",
    "                impact_frame_200hz = int(impact_frame * 2)\n",
    "                \n",
    "                # Transitional window: first half of fall initiation\n",
    "                fall_duration = impact_frame_200hz - onset_frame_200hz\n",
    "                tw_end = onset_frame_200hz + fall_duration // 2\n",
    "                \n",
    "                if tw_end > onset_frame_200hz and (tw_end - onset_frame_200hz) >= window_size:\n",
    "                    tw_data = upsampled_data[onset_frame_200hz:tw_end]\n",
    "                    \n",
    "                    windows = self.create_overlapping_windows(tw_data, window_size)\n",
    "                    for window in windows:\n",
    "                        features.append(window)\n",
    "                        labels.append('Fall_Initiation')\n",
    "        \n",
    "        return np.array(features), labels\n",
    "\n",
    "# Usage - exact replication attempt\n",
    "replicator = JainSemwalReplicator()\n",
    "\n",
    "print(\"Creating 200Hz upsampled training data (exact Jain & Semwal replication)...\")\n",
    "features_200hz, labels_200hz, metadata_200hz = replicator.create_200hz_training_data(combined_dataset)\n",
    "\n",
    "print(f\"\\nExtracted {len(features_200hz)} training windows\")jj\n",
    "if len(features_200hz) > 0:\n",
    "    print(f\"Window shape: {features_200hz[0].shape} (should be 200, 6)\")\n",
    "    print(f\"Total classes found: {len(set(labels_200hz))}\")\n",
    "    \n",
    "    # Analyze class distribution\n",
    "    class_counts = pd.Series(labels_200hz).value_counts()\n",
    "    print(\"\\nClass Distribution (200Hz):\")\n",
    "    for class_name in replicator.target_classes:\n",
    "        count = class_counts.get(class_name, 0)\n",
    "        print(f\"{class_name}: {count}\")\n",
    "else:\n",
    "    print(\"No features extracted - check data processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34493bad-758e-4688-b30f-c8fcb2d9f153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 200Hz fall phase durations...\n",
      "SSA06 T20: Fall duration = 156 frames (0.78s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA06 T21: Fall duration = 78 frames (0.39s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA06 T22: Fall duration = 132 frames (0.66s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA06 T23: Fall duration = 172 frames (0.86s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA06 T24: Fall duration = 142 frames (0.71s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA06 T25: Fall duration = 208 frames (1.04s)\n",
      "SSA06 T26: Fall duration = 152 frames (0.76s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA06 T27: Fall duration = 76 frames (0.38s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA06 T28: Fall duration = 114 frames (0.57s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA06 T29: Fall duration = 172 frames (0.86s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA06 T30: Fall duration = 150 frames (0.75s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA06 T31: Fall duration = 150 frames (0.75s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA06 T32: Fall duration = 98 frames (0.49s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA06 T33: Fall duration = 118 frames (0.59s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA06 T34: Fall duration = 130 frames (0.65s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA07 T20: Fall duration = 200 frames (1.00s)\n",
      "SSA07 T21: Fall duration = 100 frames (0.50s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA07 T22: Fall duration = 166 frames (0.83s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA07 T23: Fall duration = 174 frames (0.87s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA07 T24: Fall duration = 186 frames (0.93s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA07 T25: Fall duration = 194 frames (0.97s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA07 T26: Fall duration = 176 frames (0.88s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA07 T27: Fall duration = 96 frames (0.48s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA07 T28: Fall duration = 172 frames (0.86s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA07 T29: Fall duration = 138 frames (0.69s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA07 T30: Fall duration = 162 frames (0.81s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA07 T31: Fall duration = 158 frames (0.79s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA07 T32: Fall duration = 152 frames (0.76s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA07 T33: Fall duration = 158 frames (0.79s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA07 T34: Fall duration = 136 frames (0.68s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA08 T20: Fall duration = 206 frames (1.03s)\n",
      "SSA08 T21: Fall duration = 88 frames (0.44s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA08 T22: Fall duration = 112 frames (0.56s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA08 T23: Fall duration = 144 frames (0.72s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA08 T24: Fall duration = 94 frames (0.47s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA08 T25: Fall duration = 230 frames (1.15s)\n",
      "SSA08 T26: Fall duration = 172 frames (0.86s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA08 T27: Fall duration = 92 frames (0.46s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA08 T28: Fall duration = 166 frames (0.83s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA08 T29: Fall duration = 128 frames (0.64s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA08 T30: Fall duration = 136 frames (0.68s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA08 T31: Fall duration = 124 frames (0.62s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA08 T32: Fall duration = 108 frames (0.54s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA08 T33: Fall duration = 112 frames (0.56s)\n",
      "  Still too short for 200-sample windows\n",
      "SSA08 T34: Fall duration = 126 frames (0.63s)\n",
      "  Still too short for 200-sample windows\n"
     ]
    }
   ],
   "source": [
    "# Debug the 200Hz fall phase extraction\n",
    "fall_data = combined_dataset[combined_dataset['Is_Fall'] == True]\n",
    "labeled_falls = fall_data[fall_data['Fall_onset_frame'].notna()]\n",
    "\n",
    "print(\"Checking 200Hz fall phase durations...\")\n",
    "for (subject, task, trial), trial_data in labeled_falls.groupby(['Subject', 'Task', 'Trial']):\n",
    "    onset = int(trial_data['Fall_onset_frame'].iloc[0])\n",
    "    impact = int(trial_data['Fall_impact_frame'].iloc[0])\n",
    "    \n",
    "    # After 200Hz upsampling\n",
    "    onset_200hz = onset * 2\n",
    "    impact_200hz = impact * 2\n",
    "    fall_duration_200hz = impact_200hz - onset_200hz\n",
    "    \n",
    "    print(f\"S{subject} T{task}: Fall duration = {fall_duration_200hz} frames ({fall_duration_200hz/200:.2f}s)\")\n",
    "    if fall_duration_200hz < 200:\n",
    "        print(f\"  Still too short for 200-sample windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1d8ada8-96e0-4519-a73c-5a96c3a6d462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training data with realistic 150-sample windows...\n",
      "Processing ADL tasks with 200Hz upsampling...\n",
      "Processing fall tasks with 200Hz upsampling...\n",
      "\n",
      "Extracted 3632 training windows\n",
      "Window shape: (150, 6)\n",
      "Total classes found: 6\n",
      "\n",
      "Class Distribution (150-sample windows):\n",
      "Walking: 1652\n",
      "Jogging: 1166\n",
      "Stairs: 0\n",
      "Stumble: 606\n",
      "Fall_Recovery: 0\n",
      "Fall_Initiation: 28\n",
      "Impact: 90\n",
      "Aftermath: 90\n",
      "\n",
      "Total fall phase windows: 208\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use 150-sample windows (0.75s at 200Hz) to capture most falls\n",
    "print(\"Creating training data with realistic 150-sample windows...\")\n",
    "features_150, labels_150, metadata_150 = replicator.create_200hz_training_data(\n",
    "    combined_dataset, \n",
    "    window_size=150\n",
    ")\n",
    "\n",
    "print(f\"\\nExtracted {len(features_150)} training windows\")\n",
    "if len(features_150) > 0:\n",
    "    print(f\"Window shape: {features_150[0].shape}\")\n",
    "    print(f\"Total classes found: {len(set(labels_150))}\")\n",
    "    \n",
    "    # Analyze class distribution with realistic window size\n",
    "    class_counts = pd.Series(labels_150).value_counts()\n",
    "    print(\"\\nClass Distribution (150-sample windows):\")\n",
    "    for class_name in replicator.target_classes:\n",
    "        count = class_counts.get(class_name, 0)\n",
    "        print(f\"{class_name}: {count}\")\n",
    "        \n",
    "    # Check if we're getting more fall phase data\n",
    "    fall_classes = ['Fall_Recovery', 'Fall_Initiation', 'Impact', 'Aftermath']\n",
    "    total_fall_windows = sum(class_counts.get(cls, 0) for cls in fall_classes)\n",
    "    print(f\"\\nTotal fall phase windows: {total_fall_windows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e5a39ca-b5ad-49c0-8ff8-8c50eb7b3bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built for 6 classes: ['Aftermath' 'Fall_Initiation' 'Impact' 'Jogging' 'Stumble' 'Walking']\n"
     ]
    }
   ],
   "source": [
    "# Build CNN-LSTM for 150-sample windows\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels_150)\n",
    "\n",
    "# CNN-LSTM architecture adapted for (150, 6) input\n",
    "model = Sequential([\n",
    "    Conv1D(128, 3, activation='relu', input_shape=(150, 6)),\n",
    "    MaxPooling1D(2),\n",
    "    LSTM(256),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'), \n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "print(f\"Model built for {len(label_encoder.classes_)} classes: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ba85f66-aac5-4ae2-b143-2b0216788f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 2905 samples\n",
      "Test: 727 samples\n",
      "Epoch 1/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 62ms/step - accuracy: 0.5439 - loss: 1.1102 - val_accuracy: 0.6630 - val_loss: 0.9223\n",
      "Epoch 2/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.6571 - loss: 0.8582 - val_accuracy: 0.6974 - val_loss: 0.7733\n",
      "Epoch 3/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.6888 - loss: 0.7436 - val_accuracy: 0.7043 - val_loss: 0.7633\n",
      "Epoch 4/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - accuracy: 0.7201 - loss: 0.6605 - val_accuracy: 0.7359 - val_loss: 0.6631\n",
      "Epoch 5/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 66ms/step - accuracy: 0.7504 - loss: 0.5954 - val_accuracy: 0.7208 - val_loss: 0.7028\n",
      "Epoch 6/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - accuracy: 0.7501 - loss: 0.5902 - val_accuracy: 0.7579 - val_loss: 0.6031\n",
      "Epoch 7/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - accuracy: 0.7642 - loss: 0.5391 - val_accuracy: 0.7538 - val_loss: 0.6054\n",
      "Epoch 8/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 66ms/step - accuracy: 0.7835 - loss: 0.5089 - val_accuracy: 0.7331 - val_loss: 0.6403\n",
      "Epoch 9/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 68ms/step - accuracy: 0.7972 - loss: 0.4878 - val_accuracy: 0.7717 - val_loss: 0.5380\n",
      "Epoch 10/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.8179 - loss: 0.4381 - val_accuracy: 0.7620 - val_loss: 0.5702\n",
      "Epoch 11/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.8138 - loss: 0.4426 - val_accuracy: 0.8088 - val_loss: 0.4757\n",
      "Epoch 12/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.8396 - loss: 0.3886 - val_accuracy: 0.7854 - val_loss: 0.5801\n",
      "Epoch 13/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 66ms/step - accuracy: 0.8306 - loss: 0.4034 - val_accuracy: 0.8074 - val_loss: 0.4997\n",
      "Epoch 14/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.8392 - loss: 0.3919 - val_accuracy: 0.8157 - val_loss: 0.5351\n",
      "Epoch 15/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.8623 - loss: 0.3272 - val_accuracy: 0.8102 - val_loss: 0.4852\n",
      "Epoch 16/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.8775 - loss: 0.3066 - val_accuracy: 0.8253 - val_loss: 0.5010\n",
      "Epoch 17/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.8726 - loss: 0.3028 - val_accuracy: 0.8391 - val_loss: 0.4553\n",
      "Epoch 18/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.8726 - loss: 0.3009 - val_accuracy: 0.8253 - val_loss: 0.4466\n",
      "Epoch 19/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.8836 - loss: 0.2895 - val_accuracy: 0.7923 - val_loss: 0.5518\n",
      "Epoch 20/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.8892 - loss: 0.2772 - val_accuracy: 0.8157 - val_loss: 0.4892\n",
      "Epoch 21/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.8957 - loss: 0.2581 - val_accuracy: 0.8308 - val_loss: 0.4470\n",
      "Epoch 22/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.9133 - loss: 0.2267 - val_accuracy: 0.8501 - val_loss: 0.4593\n",
      "Epoch 23/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.9198 - loss: 0.1997 - val_accuracy: 0.8212 - val_loss: 0.5112\n",
      "Epoch 24/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.9232 - loss: 0.1964 - val_accuracy: 0.8349 - val_loss: 0.5122\n",
      "Epoch 25/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.9167 - loss: 0.2007 - val_accuracy: 0.8377 - val_loss: 0.4386\n",
      "Epoch 26/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9108 - loss: 0.2185 - val_accuracy: 0.8226 - val_loss: 0.5245\n",
      "Epoch 27/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.9239 - loss: 0.1923 - val_accuracy: 0.8006 - val_loss: 0.6048\n",
      "Epoch 28/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9336 - loss: 0.1656 - val_accuracy: 0.8116 - val_loss: 0.5123\n",
      "Epoch 29/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.9112 - loss: 0.2132 - val_accuracy: 0.8349 - val_loss: 0.4304\n",
      "Epoch 30/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.9480 - loss: 0.1357 - val_accuracy: 0.8624 - val_loss: 0.4119\n",
      "Epoch 31/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 59ms/step - accuracy: 0.9501 - loss: 0.1269 - val_accuracy: 0.8556 - val_loss: 0.4440\n",
      "Epoch 32/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.9559 - loss: 0.1198 - val_accuracy: 0.8501 - val_loss: 0.5141\n",
      "Epoch 33/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.9315 - loss: 0.1765 - val_accuracy: 0.8556 - val_loss: 0.4618\n",
      "Epoch 34/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9453 - loss: 0.1452 - val_accuracy: 0.8707 - val_loss: 0.3728\n",
      "Epoch 35/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9432 - loss: 0.1513 - val_accuracy: 0.8487 - val_loss: 0.4923\n",
      "Epoch 36/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.9422 - loss: 0.1504 - val_accuracy: 0.8404 - val_loss: 0.5159\n",
      "Epoch 37/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.9460 - loss: 0.1374 - val_accuracy: 0.8404 - val_loss: 0.5038\n",
      "Epoch 38/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.9552 - loss: 0.1171 - val_accuracy: 0.8487 - val_loss: 0.5385\n",
      "Epoch 39/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.9642 - loss: 0.1048 - val_accuracy: 0.8597 - val_loss: 0.4920\n",
      "Epoch 40/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9539 - loss: 0.1220 - val_accuracy: 0.8556 - val_loss: 0.4744\n",
      "Epoch 41/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9618 - loss: 0.0986 - val_accuracy: 0.8583 - val_loss: 0.4574\n",
      "Epoch 42/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9707 - loss: 0.0746 - val_accuracy: 0.8900 - val_loss: 0.4201\n",
      "Epoch 43/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9676 - loss: 0.0891 - val_accuracy: 0.8198 - val_loss: 0.6398\n",
      "Epoch 44/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9460 - loss: 0.1562 - val_accuracy: 0.8432 - val_loss: 0.5592\n",
      "Epoch 45/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9535 - loss: 0.1297 - val_accuracy: 0.8707 - val_loss: 0.4482\n",
      "Epoch 46/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9714 - loss: 0.0723 - val_accuracy: 0.8583 - val_loss: 0.5109\n",
      "Epoch 47/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9673 - loss: 0.0862 - val_accuracy: 0.8707 - val_loss: 0.4806\n",
      "Epoch 48/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.9725 - loss: 0.0721 - val_accuracy: 0.8501 - val_loss: 0.4990\n",
      "Epoch 49/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9793 - loss: 0.0562 - val_accuracy: 0.8666 - val_loss: 0.4908\n",
      "Epoch 50/50\n",
      "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9670 - loss: 0.0803 - val_accuracy: 0.8528 - val_loss: 0.5413\n",
      "\n",
      "Test Accuracy: 0.8528\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Aftermath       1.00      1.00      1.00        18\n",
      "Fall_Initiation       0.86      1.00      0.92         6\n",
      "         Impact       0.77      0.94      0.85        18\n",
      "        Jogging       0.88      0.85      0.86       233\n",
      "        Stumble       0.83      0.61      0.70       121\n",
      "        Walking       0.84      0.93      0.88       331\n",
      "\n",
      "       accuracy                           0.85       727\n",
      "      macro avg       0.86      0.89      0.87       727\n",
      "   weighted avg       0.85      0.85      0.85       727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_150, encoded_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=encoded_labels\n",
    ")\n",
    "j\n",
    "print(f\"Training: {len(X_train)} samples\")\n",
    "print(f\"Test: {len(X_test)} samples\")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56e9b510-b5c3-4594-a24e-4681523cecc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10 subjects for expanded dataset...\n",
      "Processing SA06...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  SA06: 119061 samples, 41138 fall samples\n",
      "Processing SA07...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  SA07: 127137 samples, 46274 fall samples\n",
      "Processing SA08...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  SA08: 126420 samples, 48617 fall samples\n",
      "Processing SA09...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  SA09: 120171 samples, 47751 fall samples\n",
      "Processing SA10...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  SA10: 128180 samples, 51496 fall samples\n",
      "Processing SA11...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  SA11: 135361 samples, 52181 fall samples\n",
      "Processing SA12...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  SA12: 106206 samples, 49696 fall samples\n",
      "Processing SA13...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  SA13: 130938 samples, 55055 fall samples\n",
      "Processing SA14...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  SA14: 126184 samples, 49424 fall samples\n",
      "Processing SA15...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  SA15: 124676 samples, 53785 fall samples\n",
      "\n",
      "Expanded dataset: 1244334 total samples\n",
      "Subjects: ['SA06', 'SA07', 'SA08', 'SA09', 'SA10', 'SA11', 'SA12', 'SA13', 'SA14', 'SA15']\n",
      "Fall vs ADL: Is_Fall\n",
      "False    748917\n",
      "True     495417\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Expand to more subjects from your available SA06-SA38 range\n",
    "target_subjects = ['SA06', 'SA07', 'SA08', 'SA09', 'SA10', 'SA11', 'SA12', 'SA13', 'SA14', 'SA15']\n",
    "\n",
    "print(\"Processing 10 subjects for expanded dataset...\")\n",
    "all_subject_datasets = []\n",
    "\n",
    "for subject_id in target_subjects:\n",
    "    try:\n",
    "        print(f\"Processing {subject_id}...\")\n",
    "        subject_data = merge_sensor_with_labels_fixed(subject_id)\n",
    "        all_subject_datasets.append(subject_data)\n",
    "        print(f\"  {subject_id}: {len(subject_data)} samples, {subject_data['Is_Fall'].sum()} fall samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error with {subject_id}: {e}\")\n",
    "\n",
    "# Combine all subjects\n",
    "if all_subject_datasets:\n",
    "    expanded_dataset = pd.concat(all_subject_datasets, ignore_index=True)\n",
    "    print(f\"\\nExpanded dataset: {len(expanded_dataset)} total samples\")\n",
    "    print(f\"Subjects: {sorted(expanded_dataset['Subject'].unique())}\")\n",
    "    print(f\"Fall vs ADL: {expanded_dataset['Is_Fall'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ab21b30-e8a0-4893-b536-37e9bf2c0366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features from expanded dataset...\n",
      "Processing ADL tasks with 200Hz upsampling...\n",
      "Processing fall tasks with 200Hz upsampling...\n",
      "Expanded feature set: 11842 windows\n",
      "Class distribution:\n",
      "Walking: 5389\n",
      "Jogging: 3893\n",
      "Stairs: 0\n",
      "Stumble: 1894\n",
      "Fall_Recovery: 0\n",
      "Fall_Initiation: 66\n",
      "Impact: 300\n",
      "Aftermath: 300\n"
     ]
    }
   ],
   "source": [
    "# Process expanded dataset (may take longer)\n",
    "print(\"Creating features from expanded dataset...\")\n",
    "features_expanded, labels_expanded, metadata_expanded = replicator.create_200hz_training_data(\n",
    "    expanded_dataset, \n",
    "    window_size=150\n",
    ")\n",
    "\n",
    "print(f\"Expanded feature set: {len(features_expanded)} windows\")\n",
    "print(\"Class distribution:\")\n",
    "class_counts = pd.Series(labels_expanded).value_counts()\n",
    "for class_name in replicator.target_classes:\n",
    "    count = class_counts.get(class_name, 0)\n",
    "    print(f\"{class_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3faf6ff0-497d-478f-92c8-e18468493945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 9473 samples\n",
      "Test: 2369 samples\n",
      "Epoch 1/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - accuracy: 0.6300 - loss: 0.9211 - val_accuracy: 0.6821 - val_loss: 0.7813\n",
      "Epoch 2/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 59ms/step - accuracy: 0.7046 - loss: 0.7096 - val_accuracy: 0.7189 - val_loss: 0.6767\n",
      "Epoch 3/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - accuracy: 0.7236 - loss: 0.6422 - val_accuracy: 0.7049 - val_loss: 0.6779\n",
      "Epoch 4/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 57ms/step - accuracy: 0.7506 - loss: 0.5814 - val_accuracy: 0.7602 - val_loss: 0.5744\n",
      "Epoch 5/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - accuracy: 0.7622 - loss: 0.5540 - val_accuracy: 0.7463 - val_loss: 0.5864\n",
      "Epoch 6/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - accuracy: 0.7716 - loss: 0.5242 - val_accuracy: 0.7733 - val_loss: 0.5247\n",
      "Epoch 7/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - accuracy: 0.7833 - loss: 0.4918 - val_accuracy: 0.7754 - val_loss: 0.5082\n",
      "Epoch 8/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 60ms/step - accuracy: 0.7901 - loss: 0.4824 - val_accuracy: 0.7801 - val_loss: 0.4967\n",
      "Epoch 9/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - accuracy: 0.8052 - loss: 0.4541 - val_accuracy: 0.7974 - val_loss: 0.4806\n",
      "Epoch 10/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - accuracy: 0.8070 - loss: 0.4433 - val_accuracy: 0.7826 - val_loss: 0.4905\n",
      "Epoch 11/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 59ms/step - accuracy: 0.8129 - loss: 0.4236 - val_accuracy: 0.8138 - val_loss: 0.4458\n",
      "Epoch 12/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 60ms/step - accuracy: 0.8197 - loss: 0.4152 - val_accuracy: 0.7885 - val_loss: 0.5012\n",
      "Epoch 13/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - accuracy: 0.8199 - loss: 0.4064 - val_accuracy: 0.8176 - val_loss: 0.4254\n",
      "Epoch 14/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 59ms/step - accuracy: 0.8301 - loss: 0.3862 - val_accuracy: 0.7847 - val_loss: 0.4987\n",
      "Epoch 15/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - accuracy: 0.8324 - loss: 0.3821 - val_accuracy: 0.8117 - val_loss: 0.4397\n",
      "Epoch 16/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 59ms/step - accuracy: 0.8387 - loss: 0.3628 - val_accuracy: 0.8231 - val_loss: 0.4344\n",
      "Epoch 17/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 59ms/step - accuracy: 0.8512 - loss: 0.3496 - val_accuracy: 0.8265 - val_loss: 0.4152\n",
      "Epoch 18/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 59ms/step - accuracy: 0.8456 - loss: 0.3594 - val_accuracy: 0.8189 - val_loss: 0.4401\n",
      "Epoch 19/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 60ms/step - accuracy: 0.8479 - loss: 0.3442 - val_accuracy: 0.8219 - val_loss: 0.4270\n",
      "Epoch 20/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 59ms/step - accuracy: 0.8585 - loss: 0.3307 - val_accuracy: 0.8223 - val_loss: 0.4554\n",
      "Epoch 21/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 59ms/step - accuracy: 0.8563 - loss: 0.3303 - val_accuracy: 0.8290 - val_loss: 0.4094\n",
      "Epoch 22/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 60ms/step - accuracy: 0.8564 - loss: 0.3298 - val_accuracy: 0.8202 - val_loss: 0.4281\n",
      "Epoch 23/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 60ms/step - accuracy: 0.8519 - loss: 0.3541 - val_accuracy: 0.8058 - val_loss: 0.4887\n",
      "Epoch 24/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 60ms/step - accuracy: 0.8561 - loss: 0.3338 - val_accuracy: 0.8219 - val_loss: 0.4512\n",
      "Epoch 25/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 60ms/step - accuracy: 0.8636 - loss: 0.3114 - val_accuracy: 0.8413 - val_loss: 0.3927\n",
      "Epoch 26/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 59ms/step - accuracy: 0.8750 - loss: 0.2916 - val_accuracy: 0.8295 - val_loss: 0.4211\n",
      "Epoch 27/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 59ms/step - accuracy: 0.8812 - loss: 0.2744 - val_accuracy: 0.8413 - val_loss: 0.3897\n",
      "Epoch 28/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 59ms/step - accuracy: 0.8867 - loss: 0.2652 - val_accuracy: 0.8396 - val_loss: 0.4112\n",
      "Epoch 29/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 59ms/step - accuracy: 0.8888 - loss: 0.2606 - val_accuracy: 0.8303 - val_loss: 0.4413\n",
      "Epoch 30/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - accuracy: 0.8857 - loss: 0.2714 - val_accuracy: 0.8404 - val_loss: 0.4132\n",
      "Epoch 31/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 59ms/step - accuracy: 0.8913 - loss: 0.2516 - val_accuracy: 0.8303 - val_loss: 0.4542\n",
      "Epoch 32/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 59ms/step - accuracy: 0.8834 - loss: 0.2686 - val_accuracy: 0.8345 - val_loss: 0.4228\n",
      "Epoch 33/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 59ms/step - accuracy: 0.8953 - loss: 0.2440 - val_accuracy: 0.8320 - val_loss: 0.4345\n",
      "Epoch 34/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 59ms/step - accuracy: 0.8952 - loss: 0.2401 - val_accuracy: 0.8354 - val_loss: 0.4315\n",
      "Epoch 35/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 59ms/step - accuracy: 0.9051 - loss: 0.2233 - val_accuracy: 0.8143 - val_loss: 0.4819\n",
      "Epoch 36/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 59ms/step - accuracy: 0.9011 - loss: 0.2319 - val_accuracy: 0.8354 - val_loss: 0.4422\n",
      "Epoch 37/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 59ms/step - accuracy: 0.9117 - loss: 0.2024 - val_accuracy: 0.8459 - val_loss: 0.4139\n",
      "Epoch 38/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 59ms/step - accuracy: 0.9083 - loss: 0.2178 - val_accuracy: 0.8417 - val_loss: 0.4244\n",
      "Epoch 39/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 59ms/step - accuracy: 0.8919 - loss: 0.2690 - val_accuracy: 0.8400 - val_loss: 0.4001\n",
      "Epoch 40/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 59ms/step - accuracy: 0.9153 - loss: 0.2074 - val_accuracy: 0.8303 - val_loss: 0.4587\n",
      "Epoch 41/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 59ms/step - accuracy: 0.9212 - loss: 0.1893 - val_accuracy: 0.8590 - val_loss: 0.4569\n",
      "Epoch 42/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - accuracy: 0.9286 - loss: 0.1724 - val_accuracy: 0.8472 - val_loss: 0.4342\n",
      "Epoch 43/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - accuracy: 0.9362 - loss: 0.1594 - val_accuracy: 0.8417 - val_loss: 0.4296\n",
      "Epoch 44/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 52ms/step - accuracy: 0.9323 - loss: 0.1667 - val_accuracy: 0.8455 - val_loss: 0.5037\n",
      "Epoch 45/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 52ms/step - accuracy: 0.9358 - loss: 0.1498 - val_accuracy: 0.8463 - val_loss: 0.4626\n",
      "Epoch 46/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 52ms/step - accuracy: 0.9352 - loss: 0.1558 - val_accuracy: 0.8556 - val_loss: 0.4388\n",
      "Epoch 47/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 52ms/step - accuracy: 0.9374 - loss: 0.1560 - val_accuracy: 0.8506 - val_loss: 0.4273\n",
      "Epoch 48/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 52ms/step - accuracy: 0.9392 - loss: 0.1487 - val_accuracy: 0.8607 - val_loss: 0.4517\n",
      "Epoch 49/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 52ms/step - accuracy: 0.9464 - loss: 0.1331 - val_accuracy: 0.8615 - val_loss: 0.4981\n",
      "Epoch 50/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 52ms/step - accuracy: 0.9433 - loss: 0.1406 - val_accuracy: 0.8573 - val_loss: 0.4556\n"
     ]
    }
   ],
   "source": [
    "# Train on expanded dataset\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels_expanded = label_encoder.fit_transform(labels_expanded)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_expanded, encoded_labels_expanded,\n",
    "    test_size=0.2, random_state=42, stratify=encoded_labels_expanded\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(X_train)} samples\")\n",
    "print(f\"Test: {len(X_test)} samples\")\n",
    "\n",
    "# Build model for expanded dataset\n",
    "model_expanded = Sequential([\n",
    "    Conv1D(128, 3, activation='relu', input_shape=(150, 6)),\n",
    "    MaxPooling1D(2),\n",
    "    LSTM(256),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model_expanded.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model_expanded.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e14f9a37-0792-4dad-bf83-63a194ad24ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all 33 subjects...\n",
      "Processing SA06...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA06: 119061 samples, 41138 fall samples\n",
      "Processing SA07...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA07: 127137 samples, 46274 fall samples\n",
      "Processing SA08...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA08: 126420 samples, 48617 fall samples\n",
      "Processing SA09...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA09: 120171 samples, 47751 fall samples\n",
      "Processing SA10...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA10: 128180 samples, 51496 fall samples\n",
      "Processing SA11...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA11: 135361 samples, 52181 fall samples\n",
      "Processing SA12...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA12: 106206 samples, 49696 fall samples\n",
      "Processing SA13...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA13: 130938 samples, 55055 fall samples\n",
      "Processing SA14...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA14: 126184 samples, 49424 fall samples\n",
      "Processing SA15...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA15: 124676 samples, 53785 fall samples\n",
      "Processing SA16...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA16: 129117 samples, 52682 fall samples\n",
      "Processing SA17...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA17: 141304 samples, 66423 fall samples\n",
      "Processing SA18...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA18: 135343 samples, 62828 fall samples\n",
      "Processing SA19...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA19: 134392 samples, 61939 fall samples\n",
      "Processing SA20...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA20: 118758 samples, 53703 fall samples\n",
      "Processing SA21...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA21: 126364 samples, 54828 fall samples\n",
      "Processing SA22...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA22: 123142 samples, 54346 fall samples\n",
      "Processing SA23...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA23: 118992 samples, 51960 fall samples\n",
      "Processing SA24...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA24: 121516 samples, 53768 fall samples\n",
      "Processing SA25...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA25: 129874 samples, 57472 fall samples\n",
      "Processing SA26...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA26: 133607 samples, 57654 fall samples\n",
      "Processing SA27...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA27: 129107 samples, 59633 fall samples\n",
      "Processing SA28...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA28: 126310 samples, 57890 fall samples\n",
      "Processing SA29...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA29: 129624 samples, 58528 fall samples\n",
      "Processing SA30...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA30: 119601 samples, 54365 fall samples\n",
      "Processing SA31...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA31: 119495 samples, 49182 fall samples\n",
      "Processing SA32...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA32: 123556 samples, 55529 fall samples\n",
      "Processing SA33...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA33: 117553 samples, 51034 fall samples\n",
      "Processing SA34...\n",
      "  ✗ SA34: [Errno 2] No such file or directory: '../fall_detection_data/label_data/SA34_label.xlsx'\n",
      "Processing SA35...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA35: 124703 samples, 57567 fall samples\n",
      "Processing SA36...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA36: 115260 samples, 52276 fall samples\n",
      "Processing SA37...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA37: 106712 samples, 48907 fall samples\n",
      "Processing SA38...\n",
      "Task mapping: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "  ✓ SA38: 126436 samples, 57476 fall samples\n",
      "\n",
      "Successfully processed: 32 subjects\n",
      "Failed subjects: ['SA34']\n"
     ]
    }
   ],
   "source": [
    "# Process all 32 subjects (SA06-SA38)\n",
    "all_subjects = [f'SA{i:02d}' for i in range(6, 39)]  # SA06 through SA38\n",
    "print(f\"Processing all {len(all_subjects)} subjects...\")\n",
    "\n",
    "successful_datasets = []\n",
    "failed_subjects = []\n",
    "\n",
    "for subject_id in all_subjects:\n",
    "    try:\n",
    "        print(f\"Processing {subject_id}...\")\n",
    "        subject_data = merge_sensor_with_labels_fixed(subject_id)\n",
    "        successful_datasets.append(subject_data)\n",
    "        print(f\"  ✓ {subject_id}: {len(subject_data)} samples, {subject_data['Is_Fall'].sum()} fall samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ {subject_id}: {e}\")\n",
    "        failed_subjects.append(subject_id)\n",
    "\n",
    "print(f\"\\nSuccessfully processed: {len(successful_datasets)} subjects\")\n",
    "print(f\"Failed subjects: {failed_subjects}\")\n",
    "\n",
    "# Combine all successful subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d70da499-813e-47fc-9165-643149088869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feature windows: Not completed\n"
     ]
    }
   ],
   "source": [
    "# Check if the feature extraction completed\n",
    "print(f\"Total feature windows: {len(features_full) if 'features_full' in locals() else 'Not completed'}\")\n",
    "\n",
    "# If completed, show class distribution\n",
    "if 'features_full' in locals():\n",
    "    class_counts = pd.Series(labels_full).value_counts()\n",
    "    print(\"Class distribution with full dataset:\")\n",
    "    for class_name in replicator.target_classes:\n",
    "        count = class_counts.get(class_name, 0)\n",
    "        print(f\"{class_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95518671-6ba6-42e0-8aa4-f90d977b4643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1: subjects 0 to 8\n",
      "Processing ADL tasks with 200Hz upsampling...\n",
      "Processing fall tasks with 200Hz upsampling...\n",
      "  Batch completed: 9397 windows\n",
      "Processing batch 2: subjects 8 to 16\n",
      "Processing ADL tasks with 200Hz upsampling...\n",
      "Processing fall tasks with 200Hz upsampling...\n",
      "  Batch completed: 9672 windows\n",
      "Processing batch 3: subjects 16 to 24\n",
      "Processing ADL tasks with 200Hz upsampling...\n",
      "Processing fall tasks with 200Hz upsampling...\n",
      "  Batch completed: 9189 windows\n",
      "Processing batch 4: subjects 24 to 32\n",
      "Processing ADL tasks with 200Hz upsampling...\n",
      "Processing fall tasks with 200Hz upsampling...\n",
      "  Batch completed: 8566 windows\n",
      "Total windows from batched processing: 36824\n"
     ]
    }
   ],
   "source": [
    "# Process subjects in batches to avoid memory issues\n",
    "batch_size = 8\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for i in range(0, len(successful_datasets), batch_size):\n",
    "    batch = successful_datasets[i:i+batch_size]\n",
    "    batch_dataset = pd.concat(batch, ignore_index=True)\n",
    "    \n",
    "    print(f\"Processing batch {i//batch_size + 1}: subjects {i} to {min(i+batch_size, len(successful_datasets))}\")\n",
    "    \n",
    "    try:\n",
    "        features_batch, labels_batch, _ = replicator.create_200hz_training_data(\n",
    "            batch_dataset, \n",
    "            window_size=150\n",
    "        )\n",
    "        all_features.append(features_batch)\n",
    "        all_labels.extend(labels_batch)\n",
    "        print(f\"  Batch completed: {len(features_batch)} windows\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Batch failed: {e}\")\n",
    "\n",
    "# Combine all batches\n",
    "if all_features:\n",
    "    features_full = np.concatenate(all_features)\n",
    "    labels_full = all_labels\n",
    "    print(f\"Total windows from batched processing: {len(features_full)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2cff06c8-df0d-427b-916f-2a6c4d41e1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution with full 32-subject dataset:\n",
      "Walking: 16552\n",
      "Jogging: 12529\n",
      "Stairs: 0\n",
      "Stumble: 5587\n",
      "Fall_Recovery: 0\n",
      "Fall_Initiation: 236\n",
      "Impact: 960\n",
      "Aftermath: 960\n",
      "\n",
      "Total fall phase windows: 2156\n"
     ]
    }
   ],
   "source": [
    "# Analyze the full dataset class distribution\n",
    "class_counts = pd.Series(labels_full).value_counts()\n",
    "print(\"Class distribution with full 32-subject dataset:\")\n",
    "for class_name in replicator.target_classes:\n",
    "    count = class_counts.get(class_name, 0)\n",
    "    print(f\"{class_name}: {count}\")\n",
    "\n",
    "print(f\"\\nTotal fall phase windows: {sum(class_counts.get(cls, 0) for cls in ['Fall_Recovery', 'Fall_Initiation', 'Impact', 'Aftermath'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f4f59e5-8746-4a2c-ae5d-56e58d1e1509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 29459\n",
      "Test samples: 7365\n",
      "Classes: ['Aftermath' 'Fall_Initiation' 'Impact' 'Jogging' 'Stumble' 'Walking']\n",
      "Epoch 1/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 88ms/step - accuracy: 0.6786 - loss: 0.7698 - val_accuracy: 0.7290 - val_loss: 0.6253\n",
      "Epoch 2/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 79ms/step - accuracy: 0.7460 - loss: 0.5884 - val_accuracy: 0.7623 - val_loss: 0.5360\n",
      "Epoch 3/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 79ms/step - accuracy: 0.7746 - loss: 0.5164 - val_accuracy: 0.7851 - val_loss: 0.4682\n",
      "Epoch 4/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 79ms/step - accuracy: 0.7842 - loss: 0.4900 - val_accuracy: 0.7984 - val_loss: 0.4592\n",
      "Epoch 5/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 79ms/step - accuracy: 0.7922 - loss: 0.4658 - val_accuracy: 0.8007 - val_loss: 0.4494\n",
      "Epoch 6/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 80ms/step - accuracy: 0.8032 - loss: 0.4390 - val_accuracy: 0.8038 - val_loss: 0.4359\n",
      "Epoch 7/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 80ms/step - accuracy: 0.8146 - loss: 0.4230 - val_accuracy: 0.8086 - val_loss: 0.4172\n",
      "Epoch 8/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 81ms/step - accuracy: 0.8148 - loss: 0.4145 - val_accuracy: 0.8129 - val_loss: 0.4176\n",
      "Epoch 9/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 80ms/step - accuracy: 0.8196 - loss: 0.4020 - val_accuracy: 0.8198 - val_loss: 0.3961\n",
      "Epoch 10/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 81ms/step - accuracy: 0.8271 - loss: 0.3854 - val_accuracy: 0.8323 - val_loss: 0.3762\n",
      "Epoch 11/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 81ms/step - accuracy: 0.8329 - loss: 0.3726 - val_accuracy: 0.8234 - val_loss: 0.4029\n",
      "Epoch 12/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 81ms/step - accuracy: 0.8330 - loss: 0.3693 - val_accuracy: 0.8114 - val_loss: 0.4139\n",
      "Epoch 13/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 81ms/step - accuracy: 0.8385 - loss: 0.3592 - val_accuracy: 0.8386 - val_loss: 0.3600\n",
      "Epoch 14/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 82ms/step - accuracy: 0.8445 - loss: 0.3423 - val_accuracy: 0.8224 - val_loss: 0.3996\n",
      "Epoch 15/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 82ms/step - accuracy: 0.8481 - loss: 0.3352 - val_accuracy: 0.8388 - val_loss: 0.3635\n",
      "Epoch 16/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 81ms/step - accuracy: 0.8548 - loss: 0.3264 - val_accuracy: 0.8430 - val_loss: 0.3690\n",
      "Epoch 17/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 81ms/step - accuracy: 0.8569 - loss: 0.3173 - val_accuracy: 0.8348 - val_loss: 0.3769\n",
      "Epoch 18/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 83ms/step - accuracy: 0.8605 - loss: 0.3129 - val_accuracy: 0.8458 - val_loss: 0.3482\n",
      "Epoch 19/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 81ms/step - accuracy: 0.8649 - loss: 0.3001 - val_accuracy: 0.8462 - val_loss: 0.3428\n",
      "Epoch 20/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 80ms/step - accuracy: 0.8671 - loss: 0.2956 - val_accuracy: 0.8474 - val_loss: 0.3432\n",
      "Epoch 21/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 81ms/step - accuracy: 0.8684 - loss: 0.2925 - val_accuracy: 0.8525 - val_loss: 0.3293\n",
      "Epoch 22/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 81ms/step - accuracy: 0.8750 - loss: 0.2808 - val_accuracy: 0.8527 - val_loss: 0.3351\n",
      "Epoch 23/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 80ms/step - accuracy: 0.8797 - loss: 0.2680 - val_accuracy: 0.8519 - val_loss: 0.3296\n",
      "Epoch 24/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 81ms/step - accuracy: 0.8802 - loss: 0.2681 - val_accuracy: 0.8508 - val_loss: 0.3306\n",
      "Epoch 25/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 81ms/step - accuracy: 0.8878 - loss: 0.2556 - val_accuracy: 0.8386 - val_loss: 0.3700\n",
      "Epoch 26/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 82ms/step - accuracy: 0.8863 - loss: 0.2559 - val_accuracy: 0.8600 - val_loss: 0.3317\n",
      "Epoch 27/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 80ms/step - accuracy: 0.8876 - loss: 0.2498 - val_accuracy: 0.8536 - val_loss: 0.3405\n",
      "Epoch 28/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 81ms/step - accuracy: 0.8934 - loss: 0.2388 - val_accuracy: 0.8481 - val_loss: 0.3618\n",
      "Epoch 29/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 81ms/step - accuracy: 0.8941 - loss: 0.2391 - val_accuracy: 0.8582 - val_loss: 0.3397\n",
      "Epoch 30/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 82ms/step - accuracy: 0.8976 - loss: 0.2309 - val_accuracy: 0.8592 - val_loss: 0.3305\n",
      "Epoch 31/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 81ms/step - accuracy: 0.9031 - loss: 0.2212 - val_accuracy: 0.8608 - val_loss: 0.3341\n",
      "Epoch 32/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 81ms/step - accuracy: 0.9086 - loss: 0.2067 - val_accuracy: 0.8616 - val_loss: 0.3576\n",
      "Epoch 33/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 80ms/step - accuracy: 0.9098 - loss: 0.2062 - val_accuracy: 0.8688 - val_loss: 0.3223\n",
      "Epoch 34/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 81ms/step - accuracy: 0.9105 - loss: 0.2027 - val_accuracy: 0.8701 - val_loss: 0.3222\n",
      "Epoch 35/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 79ms/step - accuracy: 0.9126 - loss: 0.2003 - val_accuracy: 0.8621 - val_loss: 0.3371\n",
      "Epoch 36/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 77ms/step - accuracy: 0.9137 - loss: 0.1956 - val_accuracy: 0.8646 - val_loss: 0.3364\n",
      "Epoch 37/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 80ms/step - accuracy: 0.9175 - loss: 0.1877 - val_accuracy: 0.8646 - val_loss: 0.3368\n",
      "Epoch 38/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 82ms/step - accuracy: 0.9241 - loss: 0.1741 - val_accuracy: 0.8669 - val_loss: 0.3499\n",
      "Epoch 39/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 82ms/step - accuracy: 0.9213 - loss: 0.1785 - val_accuracy: 0.8650 - val_loss: 0.3530\n",
      "Epoch 40/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 78ms/step - accuracy: 0.9273 - loss: 0.1691 - val_accuracy: 0.8629 - val_loss: 0.3803\n",
      "Epoch 41/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 83ms/step - accuracy: 0.9275 - loss: 0.1641 - val_accuracy: 0.8673 - val_loss: 0.3534\n",
      "Epoch 42/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 81ms/step - accuracy: 0.9310 - loss: 0.1579 - val_accuracy: 0.8716 - val_loss: 0.3334\n",
      "Epoch 43/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 81ms/step - accuracy: 0.9341 - loss: 0.1503 - val_accuracy: 0.8680 - val_loss: 0.3671\n",
      "Epoch 44/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 82ms/step - accuracy: 0.9342 - loss: 0.1553 - val_accuracy: 0.8597 - val_loss: 0.3852\n",
      "Epoch 45/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 79ms/step - accuracy: 0.9349 - loss: 0.1542 - val_accuracy: 0.8752 - val_loss: 0.3518\n",
      "Epoch 46/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 82ms/step - accuracy: 0.9380 - loss: 0.1433 - val_accuracy: 0.8687 - val_loss: 0.3613\n",
      "Epoch 47/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 82ms/step - accuracy: 0.9429 - loss: 0.1349 - val_accuracy: 0.8686 - val_loss: 0.3865\n",
      "Epoch 48/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 83ms/step - accuracy: 0.9447 - loss: 0.1307 - val_accuracy: 0.8657 - val_loss: 0.3636\n",
      "Epoch 49/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 83ms/step - accuracy: 0.9457 - loss: 0.1289 - val_accuracy: 0.8702 - val_loss: 0.3646\n",
      "Epoch 50/50\n",
      "\u001b[1m461/461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 84ms/step - accuracy: 0.9480 - loss: 0.1223 - val_accuracy: 0.8767 - val_loss: 0.3717\n"
     ]
    }
   ],
   "source": [
    "# Build the model for the full dataset\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels_full = label_encoder.fit_transform(labels_full)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_full, encoded_labels_full,\n",
    "    test_size=0.2, random_state=42, stratify=encoded_labels_full\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")\n",
    "\n",
    "# Build CNN-LSTM model\n",
    "model_full = Sequential([\n",
    "    Conv1D(128, 3, activation='relu', input_shape=(150, 6)),\n",
    "    MaxPooling1D(2),\n",
    "    LSTM(256),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model_full.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model_full.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab5a7a1-4a5a-4eac-8d64-9d0c6ccdec78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
